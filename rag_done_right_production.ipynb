{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indranildchandra/rag-done-right-in-production/blob/main/rag_done_right_production.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "title-cell",
      "metadata": {
        "id": "title-cell"
      },
      "source": [
        "# RAG Done Right in Production\n",
        "### Demo Notebook — Zerodha Support FAQ Corpus\n",
        "\n",
        "**Talk:** Can you RAG like a Pro? · Indranil Chandra, ML & Data Architect, Upstox\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is a **self-contained production RAG demo** that runs entirely inside Google Colab — no external API keys required. It builds a real retrieval pipeline on top of Zerodha's public support FAQ corpus.\n",
        "\n",
        "**What this covers, in order:**\n",
        "\n",
        "| # | Section | What you will observe |\n",
        "|---|---------|----------------------|\n",
        "| 1 | **Corpus Ingestion** | Scrape + parse Zerodha support FAQs |\n",
        "| 2 | **Chunking** | Semantic chunking vs naive fixed-size |\n",
        "| 3 | **Qdrant (in-memory)** | Embed + index with `all-MiniLM-L6-v2` |\n",
        "| 4 | **Hybrid Search** | BM25 + dense retrieval merged with RRF |\n",
        "| 5 | **Cross-Encoder Reranking** | Two-stage pipeline: recall then precision |\n",
        "| 6 | **Adaptive-k** | Score-gap boundary vs fixed top-k |\n",
        "| 7 | **Generation + Citations** | LLM answer grounded with source attribution |\n",
        "| 8 | **Evaluation** | Hit rate, Faithfulness, Hallucination detection |\n",
        "\n",
        "> **Runtime:** GPU not required. CPU is sufficient. Estimated full run: ~8–12 minutes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-0",
      "metadata": {
        "id": "section-0"
      },
      "source": [
        "---\n",
        "## 0 · Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-deps",
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install qdrant-client sentence-transformers rank-bm25 \\\n",
        "             beautifulsoup4 requests transformers \\\n",
        "             openai tiktoken tqdm colorama --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os, re, json, time, random, hashlib, warnings\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import (\n",
        "    Distance, VectorParams, PointStruct,\n",
        "    Filter, FieldCondition, MatchValue\n",
        ")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ── Colour helpers for readable notebook output ────────────────────\n",
        "from colorama import Fore, Style, init as colorama_init\n",
        "colorama_init(autoreset=True)\n",
        "\n",
        "def hdr(msg):  print(f\"\\n{Fore.CYAN}{Style.BRIGHT}{'─'*60}\\n  {msg}\\n{'─'*60}{Style.RESET_ALL}\")\n",
        "def ok(msg):   print(f\"{Fore.GREEN}✓  {msg}{Style.RESET_ALL}\")\n",
        "def info(msg): print(f\"{Fore.YELLOW}ℹ  {msg}{Style.RESET_ALL}\")\n",
        "def err(msg):  print(f\"{Fore.RED}✗  {msg}{Style.RESET_ALL}\")\n",
        "\n",
        "ok(\"Imports complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "---\n",
        "## 1 · Corpus Ingestion — Zerodha Support FAQ Scraper\n",
        "\n",
        "The scraper works in **three passes**:\n",
        "\n",
        "1. **Category discovery** — read the homepage sidebar to find all section URLs  \n",
        "2. **Article link extraction** — for each category page, collect article hrefs  \n",
        "3. **Article fetch + parse** — extract `<h1>` title and body text, strip nav/footer boilerplate  \n",
        "\n",
        "We scrape politely: 0.4s delay between requests, max 120 articles, 3 retries with backoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scraper-config",
      "metadata": {
        "id": "scraper-config"
      },
      "outputs": [],
      "source": [
        "# ── Scraper configuration ─────────────────────────────────────────\n",
        "BASE_URL      = \"https://support.zerodha.com\"\n",
        "MAX_ARTICLES  = 120       # cap for demo — raise to 500+ for production\n",
        "DELAY_SEC     = 0.4       # polite crawl delay\n",
        "RETRIES       = 3\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (compatible; RAGDemoBot/1.0; \"\n",
        "        \"+https://github.com/indranilchandra/rag-demo)\"\n",
        "    ),\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "}\n",
        "\n",
        "# Category sections to crawl — covers trading, account, funds, coin\n",
        "SEED_CATEGORIES = [\n",
        "    \"/category/trading-and-markets/trading-faqs\",\n",
        "    \"/category/trading-and-markets/margins\",\n",
        "    \"/category/trading-and-markets/charts-and-orders\",\n",
        "    \"/category/trading-and-markets/general-kite\",\n",
        "    \"/category/your-zerodha-account/your-profile\",\n",
        "    \"/category/your-zerodha-account/account-modification-and-segment-addition\",\n",
        "    \"/category/funds/adding-funds\",\n",
        "    \"/category/funds/fund-withdrawal\",\n",
        "    \"/category/mutual-funds/understanding-mutual-funds\",\n",
        "    \"/category/mutual-funds/payments-and-orders\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scraper-functions",
      "metadata": {
        "id": "scraper-functions"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ZerodhaArticle:\n",
        "    url:      str\n",
        "    title:    str\n",
        "    body:     str\n",
        "    category: str\n",
        "    doc_id:   str = field(init=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.doc_id = hashlib.md5(self.url.encode()).hexdigest()[:12]\n",
        "\n",
        "\n",
        "def _get(url: str, retries: int = RETRIES) -> Optional[BeautifulSoup]:\n",
        "    \"\"\"Fetch a URL and return a BeautifulSoup object. Returns None on failure.\"\"\"\n",
        "    full = url if url.startswith(\"http\") else BASE_URL + url\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            r = requests.get(full, headers=HEADERS, timeout=15)\n",
        "            if r.status_code == 200:\n",
        "                time.sleep(DELAY_SEC)\n",
        "                return BeautifulSoup(r.text, \"html.parser\")\n",
        "            elif r.status_code == 429:\n",
        "                wait = 2 ** (attempt + 1)\n",
        "                info(f\"Rate limited — waiting {wait}s\")\n",
        "                time.sleep(wait)\n",
        "        except requests.RequestException as e:\n",
        "            time.sleep(2 ** attempt)\n",
        "    return None\n",
        "\n",
        "\n",
        "def _extract_article_links(category_url: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    From a category page, collect all /articles/ hrefs.\n",
        "    Zerodha renders article links as <a href=\"/category/.../articles/slug\">\n",
        "    \"\"\"\n",
        "    soup = _get(category_url)\n",
        "    if not soup:\n",
        "        return []\n",
        "    links = set()\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if \"/articles/\" in href and href.startswith(\"/category/\"):\n",
        "            links.add(href)\n",
        "    return list(links)\n",
        "\n",
        "\n",
        "# Boilerplate patterns to strip from article body\n",
        "_BOILERPLATE = re.compile(\n",
        "    r\"(Updates|Education|Utilities|Support Portal|Related articles|Quick links\"\n",
        "    r\"|Signup|About|Products|Pricing|Zerodha Broking|© 20\\d\\d)\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "\n",
        "def _parse_article(href: str, category: str) -> Optional[ZerodhaArticle]:\n",
        "    \"\"\"\n",
        "    Fetch and parse a single article page.\n",
        "    Strategy:\n",
        "      - Title  → <h1> tag\n",
        "      - Body   → all <p> tags after the <h1>, before 'Related articles'\n",
        "    \"\"\"\n",
        "    soup = _get(href)\n",
        "    if not soup:\n",
        "        return None\n",
        "\n",
        "    h1 = soup.find(\"h1\")\n",
        "    if not h1:\n",
        "        return None\n",
        "    title = h1.get_text(strip=True)\n",
        "\n",
        "    # Collect paragraphs that appear after the <h1>\n",
        "    paragraphs = []\n",
        "    in_body = False\n",
        "    for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"]):\n",
        "        if tag == h1:\n",
        "            in_body = True\n",
        "            continue\n",
        "        if not in_body:\n",
        "            continue\n",
        "        text = tag.get_text(\" \", strip=True)\n",
        "        if not text or len(text) < 15:\n",
        "            continue\n",
        "        # Stop at 'Related articles' section\n",
        "        if re.search(r\"related articles\", text, re.IGNORECASE):\n",
        "            break\n",
        "        if _BOILERPLATE.search(text):\n",
        "            continue\n",
        "        paragraphs.append(text)\n",
        "\n",
        "    body = \" \".join(paragraphs).strip()\n",
        "    if len(body) < 80:   # skip stubs\n",
        "        return None\n",
        "\n",
        "    return ZerodhaArticle(\n",
        "        url=BASE_URL + href,\n",
        "        title=title,\n",
        "        body=body,\n",
        "        category=category,\n",
        "    )\n",
        "\n",
        "\n",
        "ok(\"Scraper functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run-scraper",
      "metadata": {
        "id": "run-scraper"
      },
      "outputs": [],
      "source": [
        "hdr(\"Scraping Zerodha Support FAQs\")\n",
        "\n",
        "all_links: Dict[str, str] = {}   # href → category label\n",
        "\n",
        "for cat_url in SEED_CATEGORIES:\n",
        "    label = cat_url.split(\"/\")[-1].replace(\"-\", \" \").title()\n",
        "    links = _extract_article_links(cat_url)\n",
        "    for lnk in links:\n",
        "        all_links[lnk] = label\n",
        "    info(f\"{label}: {len(links)} article links found\")\n",
        "\n",
        "# Shuffle so we get a diverse sample if we hit the cap\n",
        "all_link_items = list(all_links.items())\n",
        "random.seed(42)\n",
        "random.shuffle(all_link_items)\n",
        "all_link_items = all_link_items[:MAX_ARTICLES]\n",
        "\n",
        "ok(f\"Total unique article links to fetch: {len(all_link_items)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fetch-articles",
      "metadata": {
        "id": "fetch-articles"
      },
      "outputs": [],
      "source": [
        "articles: List[ZerodhaArticle] = []\n",
        "\n",
        "for href, category in tqdm(all_link_items, desc=\"Fetching articles\"):\n",
        "    art = _parse_article(href, category)\n",
        "    if art:\n",
        "        articles.append(art)\n",
        "\n",
        "ok(f\"Successfully parsed {len(articles)} articles\")\n",
        "\n",
        "# Quick sanity check — show 3 samples\n",
        "print()\n",
        "for art in articles[:3]:\n",
        "    print(f\"  [{art.category}]  {art.title}\")\n",
        "    print(f\"  Body ({len(art.body)} chars): {art.body[:120]}...\")\n",
        "    print(f\"  URL: {art.url}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-corpus",
      "metadata": {
        "id": "save-corpus"
      },
      "outputs": [],
      "source": [
        "# Persist to disk — so you can reload without re-scraping\n",
        "corpus_path = \"/content/zerodha_faqs.json\"\n",
        "with open(corpus_path, \"w\") as f:\n",
        "    json.dump(\n",
        "        [{\"doc_id\": a.doc_id, \"url\": a.url, \"title\": a.title,\n",
        "          \"body\": a.body, \"category\": a.category} for a in articles],\n",
        "        f, indent=2\n",
        "    )\n",
        "ok(f\"Corpus saved → {corpus_path}  ({os.path.getsize(corpus_path)//1024} KB)\")\n",
        "\n",
        "# Category distribution\n",
        "from collections import Counter\n",
        "dist = Counter(a.category for a in articles)\n",
        "print(\"\\nCategory distribution:\")\n",
        "for cat, cnt in dist.most_common():\n",
        "    bar = \"█\" * cnt\n",
        "    print(f\"  {cat:<50} {cnt:>3}  {bar}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "---\n",
        "## 2 · Chunking Strategy\n",
        "\n",
        "**The problem with fixed-size chunking:** A 512-token window that splits mid-sentence loses the semantic unit. Two adjacent chunks end up containing half a thought each — both retrieve poorly.\n",
        "\n",
        "**What we use here:** Sentence-boundary aware chunking with a configurable overlap. Each chunk respects sentence boundaries, with a sliding window overlap to preserve cross-sentence context.\n",
        "\n",
        "For a support FAQ corpus specifically, most articles are short enough (200–600 tokens) that we treat each article as 1–3 chunks max. The overlap handles the edge case where a key fact straddles a sentence boundary.\n",
        "\n",
        "**Production note:** For longer documents (regulatory PDFs, annual reports), switch to hierarchical chunking: paragraph-level for indexing, sentence-level for reranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chunker",
      "metadata": {
        "id": "chunker"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Chunk:\n",
        "    chunk_id:   str\n",
        "    doc_id:     str\n",
        "    title:      str\n",
        "    text:       str          # chunk text that gets embedded\n",
        "    full_text:  str          # full article body — for citation display\n",
        "    url:        str\n",
        "    category:   str\n",
        "    chunk_idx:  int\n",
        "\n",
        "\n",
        "def sentence_aware_chunk(\n",
        "    article: ZerodhaArticle,\n",
        "    max_chars: int = 800,\n",
        "    overlap_chars: int = 120,\n",
        ") -> List[Chunk]:\n",
        "    \"\"\"\n",
        "    Split article body into overlapping sentence-boundary-aware chunks.\n",
        "    Each chunk prepends the article title for embedding context.\n",
        "    \"\"\"\n",
        "    # Split on sentence boundaries\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', article.body.strip())\n",
        "    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "\n",
        "    chunks = []\n",
        "    buf, buf_len = [], 0\n",
        "    idx = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        if buf_len + len(sent) > max_chars and buf:\n",
        "            text = \" \".join(buf)\n",
        "            chunks.append(Chunk(\n",
        "                chunk_id  = f\"{article.doc_id}_{idx}\",\n",
        "                doc_id    = article.doc_id,\n",
        "                title     = article.title,\n",
        "                text      = f\"{article.title}. {text}\",\n",
        "                full_text = article.body,\n",
        "                url       = article.url,\n",
        "                category  = article.category,\n",
        "                chunk_idx = idx,\n",
        "            ))\n",
        "            idx += 1\n",
        "            # Carry over tail sentences for overlap\n",
        "            overlap_buf, overlap_len = [], 0\n",
        "            for s in reversed(buf):\n",
        "                if overlap_len + len(s) < overlap_chars:\n",
        "                    overlap_buf.insert(0, s)\n",
        "                    overlap_len += len(s)\n",
        "                else:\n",
        "                    break\n",
        "            buf, buf_len = overlap_buf, overlap_len\n",
        "\n",
        "        buf.append(sent)\n",
        "        buf_len += len(sent)\n",
        "\n",
        "    # Flush remainder\n",
        "    if buf:\n",
        "        text = \" \".join(buf)\n",
        "        chunks.append(Chunk(\n",
        "            chunk_id  = f\"{article.doc_id}_{idx}\",\n",
        "            doc_id    = article.doc_id,\n",
        "            title     = article.title,\n",
        "            text      = f\"{article.title}. {text}\",\n",
        "            full_text = article.body,\n",
        "            url       = article.url,\n",
        "            category  = article.category,\n",
        "            chunk_idx = idx,\n",
        "        ))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Build chunk corpus\n",
        "all_chunks: List[Chunk] = []\n",
        "for art in articles:\n",
        "    all_chunks.extend(sentence_aware_chunk(art))\n",
        "\n",
        "ok(f\"{len(articles)} articles → {len(all_chunks)} chunks\")\n",
        "avg_len = sum(len(c.text) for c in all_chunks) / len(all_chunks)\n",
        "info(f\"Average chunk length: {avg_len:.0f} chars\")\n",
        "\n",
        "# Show chunking on a sample article\n",
        "sample_art = max(articles, key=lambda a: len(a.body))\n",
        "sample_chunks = sentence_aware_chunk(sample_art)\n",
        "print(f\"\\nSample article: '{sample_art.title}'\")\n",
        "print(f\"Body length: {len(sample_art.body)} chars → {len(sample_chunks)} chunks\")\n",
        "for i, c in enumerate(sample_chunks):\n",
        "    print(f\"  Chunk {i}: {len(c.text)} chars | '{c.text[:80]}...'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "---\n",
        "## 3 · Qdrant In-Memory Vector Store\n",
        "\n",
        "We use **Qdrant's in-memory mode** — no Docker, no external service, no API key. Same Python client API as production Qdrant Cloud. This means your demo code is identical to what you would run in prod — just swap `QdrantClient(':memory:')` for `QdrantClient(url='https://your-cluster.qdrant.io', api_key='...')`.\n",
        "\n",
        "**Embedding model:** `all-MiniLM-L6-v2`  \n",
        "- 384 dimensions, 22M parameters, ~80ms per 100 chunks on CPU  \n",
        "- Production note: for a finance domain, fine-tuned models on SEBI/NSE corpus improve retrieval recall by 10–15%  \n",
        "- 768-dim models (MPNet) give diminishing returns past this corpus size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "embedding-model",
      "metadata": {
        "id": "embedding-model"
      },
      "outputs": [],
      "source": [
        "hdr(\"Loading embedding model\")\n",
        "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "EMBED_DIM = embed_model.get_sentence_embedding_dimension()\n",
        "ok(f\"Model: {EMBED_MODEL_NAME} | Dim: {EMBED_DIM}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qdrant-setup",
      "metadata": {
        "id": "qdrant-setup"
      },
      "outputs": [],
      "source": [
        "hdr(\"Setting up Qdrant in-memory + indexing chunks\")\n",
        "\n",
        "COLLECTION = \"zerodha_faqs\"\n",
        "\n",
        "# In-memory Qdrant — identical API to Qdrant Cloud\n",
        "qdrant = QdrantClient(\":memory:\")\n",
        "qdrant.create_collection(\n",
        "    collection_name=COLLECTION,\n",
        "    vectors_config=VectorParams(size=EMBED_DIM, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "# Embed all chunks in batches\n",
        "BATCH_SIZE = 64\n",
        "texts = [c.text for c in all_chunks]\n",
        "all_embeddings = []\n",
        "\n",
        "for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding\"):\n",
        "    batch = texts[i : i + BATCH_SIZE]\n",
        "    embs  = embed_model.encode(batch, normalize_embeddings=True)\n",
        "    all_embeddings.extend(embs)\n",
        "\n",
        "all_embeddings = np.array(all_embeddings)\n",
        "ok(f\"Embeddings shape: {all_embeddings.shape}\")\n",
        "\n",
        "# Upsert into Qdrant\n",
        "points = [\n",
        "    PointStruct(\n",
        "        id=i,\n",
        "        vector=all_embeddings[i].tolist(),\n",
        "        payload={\n",
        "            \"chunk_id\":  c.chunk_id,\n",
        "            \"doc_id\":    c.doc_id,\n",
        "            \"title\":     c.title,\n",
        "            \"text\":      c.text,\n",
        "            \"full_text\": c.full_text,\n",
        "            \"url\":       c.url,\n",
        "            \"category\":  c.category,\n",
        "            \"chunk_idx\": c.chunk_idx,\n",
        "        }\n",
        "    )\n",
        "    for i, c in enumerate(all_chunks)\n",
        "]\n",
        "\n",
        "# Upload in batches of 256\n",
        "for i in tqdm(range(0, len(points), 256), desc=\"Uploading to Qdrant\"):\n",
        "    qdrant.upsert(collection_name=COLLECTION, points=points[i:i+256])\n",
        "\n",
        "collection_info = qdrant.get_collection(COLLECTION)\n",
        "ok(f\"Qdrant collection '{COLLECTION}' ready — {collection_info.points_count} vectors indexed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "---\n",
        "## 4 · Hybrid Search — BM25 + Dense + RRF Fusion\n",
        "\n",
        "**Why not dense-only?**  \n",
        "Dense retrieval is strong on semantic similarity. It misses exact-term matches — ticker symbols, product names, regulatory codes. BM25 nails exact matches but fails on paraphrase. Neither alone is sufficient for a support FAQ system where users ask both `\"what is MTF\"` (semantic) and `\"MTF margin requirement\"` (keyword).\n",
        "\n",
        "**Reciprocal Rank Fusion (RRF):**  \n",
        "Score = Σ 1/(k + rank_i) where k=60 is a smoothing constant. RRF is rank-based — it does not require score normalization between BM25 and cosine similarity, which makes it robust without hyperparameter tuning.\n",
        "\n",
        "**Production trade-off:**  \n",
        "BM25 runs on CPU in memory. Dense retrieval hits the vector index. For 100k+ chunks, the bottleneck shifts to BM25 RAM — at that scale, replace BM25 with Elasticsearch sparse vectors or Qdrant's built-in sparse support (FastEmbed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bm25-setup",
      "metadata": {
        "id": "bm25-setup"
      },
      "outputs": [],
      "source": [
        "# Build BM25 index over tokenized chunk texts\n",
        "hdr(\"Building BM25 sparse index\")\n",
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    \"\"\"Simple whitespace + lowercase tokenizer. Good enough for BM25.\"\"\"\n",
        "    return re.findall(r\"[a-z0-9]+\", text.lower())\n",
        "\n",
        "tokenized_corpus = [tokenize(c.text) for c in all_chunks]\n",
        "bm25_index = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "ok(f\"BM25 index built over {len(tokenized_corpus)} chunks\")\n",
        "info(\"Average doc length (BM25): \" + str(round(bm25_index.avgdl, 1)) + \" tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hybrid-search-fn",
      "metadata": {
        "id": "hybrid-search-fn"
      },
      "outputs": [],
      "source": [
        "def dense_search(query: str, top_k: int = 30) -> List[Tuple[int, float]]:\n",
        "    \"\"\"\n",
        "    Query Qdrant for top_k nearest neighbours.\n",
        "    Returns list of (point_index, cosine_score).\n",
        "    \"\"\"\n",
        "    q_emb = embed_model.encode([query], normalize_embeddings=True)[0]\n",
        "    results = qdrant.query_points(\n",
        "        collection_name=COLLECTION,\n",
        "        query=q_emb.tolist(),\n",
        "        limit=top_k,\n",
        "    )\n",
        "    points = results[0] if isinstance(results, tuple) else results.points\n",
        "    return [(r.id, r.score) for r in points]\n",
        "\n",
        "\n",
        "def bm25_search(query: str, top_k: int = 30) -> List[Tuple[int, float]]:\n",
        "    \"\"\"\n",
        "    BM25 retrieval. Returns list of (chunk_index, bm25_score).\n",
        "    \"\"\"\n",
        "    q_tokens = tokenize(query)\n",
        "    scores = bm25_index.get_scores(q_tokens)\n",
        "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "    return [(int(idx), float(scores[idx])) for idx in top_indices if scores[idx] > 0]\n",
        "\n",
        "\n",
        "def reciprocal_rank_fusion(\n",
        "    dense_hits:  List[Tuple[int, float]],\n",
        "    sparse_hits: List[Tuple[int, float]],\n",
        "    k: int = 60,\n",
        "    top_k: int = 20,\n",
        ") -> List[Tuple[int, float]]:\n",
        "    \"\"\"\n",
        "    Fuse two ranked lists using RRF.\n",
        "    k=60 is the standard smoothing constant from the original RRF paper.\n",
        "    \"\"\"\n",
        "    rrf_scores: Dict[int, float] = {}\n",
        "\n",
        "    for rank, (idx, _) in enumerate(dense_hits):\n",
        "        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (k + rank + 1)\n",
        "\n",
        "    for rank, (idx, _) in enumerate(sparse_hits):\n",
        "        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (k + rank + 1)\n",
        "\n",
        "    fused = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return fused[:top_k]\n",
        "\n",
        "\n",
        "def hybrid_search(query: str, stage1_k: int = 30) -> List[Tuple[Chunk, float]]:\n",
        "    \"\"\"\n",
        "    Full hybrid search pipeline.\n",
        "    Returns list of (Chunk, rrf_score) sorted by descending RRF score.\n",
        "    \"\"\"\n",
        "    dense_hits  = dense_search(query, top_k=stage1_k)\n",
        "    sparse_hits = bm25_search(query, top_k=stage1_k)\n",
        "    fused       = reciprocal_rank_fusion(dense_hits, sparse_hits, top_k=stage1_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, score in fused:\n",
        "        chunk = all_chunks[idx]\n",
        "        results.append((chunk, score))\n",
        "    return results\n",
        "\n",
        "\n",
        "ok(\"Hybrid search functions ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hybrid-search-demo",
      "metadata": {
        "id": "hybrid-search-demo"
      },
      "outputs": [],
      "source": [
        "# ── Demo: compare Dense-only vs Hybrid ───────────────────────────\n",
        "hdr(\"Demo: Dense-only vs Hybrid Search\")\n",
        "DEMO_QUERY = \"how do I withdraw funds from my Zerodha account\"\n",
        "\n",
        "print(f\"Query: '{DEMO_QUERY}'\\n\")\n",
        "\n",
        "dense_only  = dense_search(DEMO_QUERY, top_k=5)\n",
        "hybrid_hits = hybrid_search(DEMO_QUERY, stage1_k=30)\n",
        "\n",
        "print(f\"{Fore.CYAN}{'─'*28} Dense-only top-5 {'─'*14}{Style.RESET_ALL}\")\n",
        "for rank, (idx, score) in enumerate(dense_only, 1):\n",
        "    c = all_chunks[idx]\n",
        "    print(f\"  {rank}. [{score:.3f}] {c.title}\")\n",
        "\n",
        "print(f\"\\n{Fore.CYAN}{'─'*28} Hybrid top-5 (RRF) {'─'*12}{Style.RESET_ALL}\")\n",
        "for rank, (chunk, score) in enumerate(hybrid_hits[:5], 1):\n",
        "    print(f\"  {rank}. [{score:.4f}] {chunk.title}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "---\n",
        "## 5 · Cross-Encoder Reranking\n",
        "\n",
        "**The two-tower gap:** Bi-encoders (like MiniLM) encode query and document independently — they cannot model fine-grained token-level interactions between the two. This is fast but imprecise.\n",
        "\n",
        "**Cross-encoders** process the (query, document) pair jointly through the full attention stack. They see every token of the query attending to every token of the document. Dramatically more accurate — but O(n) in inference cost.\n",
        "\n",
        "**The production pattern:** Use bi-encoder to recall 20–30 candidates cheaply, then cross-encoder to rerank to final top-5. You pay cross-encoder cost only on the shortlist, not the full corpus.\n",
        "\n",
        "**Latency budget:** With `ms-marco-MiniLM-L-6-v2` on CPU:  \n",
        "- Bi-encoder recall on 500 chunks: ~80ms  \n",
        "- Cross-encoder on 20 candidates: ~120ms  \n",
        "- Total retrieval budget: ~200ms — well inside a 1.5s response SLA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cross-encoder",
      "metadata": {
        "id": "cross-encoder"
      },
      "outputs": [],
      "source": [
        "hdr(\"Loading Cross-Encoder reranker\")\n",
        "CE_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "cross_encoder = CrossEncoder(CE_MODEL_NAME)\n",
        "ok(f\"Cross-encoder: {CE_MODEL_NAME}\")\n",
        "\n",
        "\n",
        "def rerank(\n",
        "    query: str,\n",
        "    candidates: List[Tuple[Chunk, float]],\n",
        "    final_k: int = 5,\n",
        ") -> List[Tuple[Chunk, float]]:\n",
        "    \"\"\"\n",
        "    Rerank hybrid search candidates with the cross-encoder.\n",
        "    Input:  candidates from hybrid_search (shortlist of ~20–30)\n",
        "    Output: top final_k chunks with cross-encoder relevance scores\n",
        "    \"\"\"\n",
        "    if not candidates:\n",
        "        return []\n",
        "\n",
        "    pairs  = [(query, c.text) for c, _ in candidates]\n",
        "    scores = cross_encoder.predict(pairs)\n",
        "\n",
        "    ranked = sorted(\n",
        "        zip([c for c, _ in candidates], scores),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "    return ranked[:final_k]\n",
        "\n",
        "\n",
        "# ── Demo ─────────────────────────────────────────────────────────\n",
        "t0 = time.time()\n",
        "candidates = hybrid_search(DEMO_QUERY, stage1_k=25)\n",
        "reranked   = rerank(DEMO_QUERY, candidates, final_k=5)\n",
        "elapsed    = time.time() - t0\n",
        "\n",
        "print(f\"\\nQuery: '{DEMO_QUERY}'\")\n",
        "print(f\"Retrieval pipeline total: {elapsed*1000:.0f}ms\\n\")\n",
        "\n",
        "print(f\"{Fore.CYAN}{'─'*30} After cross-encoder reranking {'─'*1}{Style.RESET_ALL}\")\n",
        "for rank, (chunk, score) in enumerate(reranked, 1):\n",
        "    print(f\"  {rank}. [CE={score:.3f}] {chunk.title}\")\n",
        "    print(f\"       {chunk.text[:120]}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "---\n",
        "## 6 · Adaptive-k Retrieval\n",
        "\n",
        "**The fixed-k problem:** Setting `top_k=5` works well for narrow queries. For ambiguous queries — `\"margin\"` could mean MTF margin, intraday margin, or Options margin — you need more context. But padding every query with top-10 inflates prompt cost and dilutes the signal fed to the LLM.\n",
        "\n",
        "**Adaptive-k** uses the distribution of cross-encoder scores to find the natural relevance cliff:\n",
        "\n",
        "```\n",
        "k = argmax( score_i − score_{i+1} )\n",
        "```\n",
        "\n",
        "The largest consecutive score drop is the boundary between relevant and irrelevant. Everything above the cliff goes into context. Everything below gets dropped — no LLM sees it, no token budget is consumed.\n",
        "\n",
        "**When it fails:** Flat score curves with no clear cliff — usually when the query is highly ambiguous or the embedding space is too compressed. Mitigate with a minimum k of 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaptive-k",
      "metadata": {
        "id": "adaptive-k"
      },
      "outputs": [],
      "source": [
        "def adaptive_k(\n",
        "    ranked_chunks: List[Tuple[Chunk, float]],\n",
        "    min_k: int = 2,\n",
        "    max_k: int = 8,\n",
        ") -> List[Tuple[Chunk, float]]:\n",
        "    \"\"\"\n",
        "    Apply adaptive-k boundary detection on cross-encoder scored candidates.\n",
        "    Returns only the chunks above the steepest score drop.\n",
        "\n",
        "    k = argmax(score_i - score_{i+1})\n",
        "    \"\"\"\n",
        "    if len(ranked_chunks) <= min_k:\n",
        "        return ranked_chunks\n",
        "\n",
        "    scores = [s for _, s in ranked_chunks[:max_k]]\n",
        "\n",
        "    # Compute consecutive differences\n",
        "    gaps = [scores[i] - scores[i+1] for i in range(len(scores)-1)]\n",
        "\n",
        "    if not gaps:\n",
        "        return ranked_chunks[:min_k]\n",
        "\n",
        "    cliff_idx = int(np.argmax(gaps))   # index of steepest drop\n",
        "    k = max(min_k, cliff_idx + 1)      # include everything above the cliff\n",
        "\n",
        "    return ranked_chunks[:k]\n",
        "\n",
        "\n",
        "# ── Demo: adaptive-k vs fixed-k on several query types ───────────\n",
        "hdr(\"Demo: Adaptive-k vs Fixed-k\")\n",
        "\n",
        "test_queries = [\n",
        "    \"how to place a stop loss order\",       # narrow — expect k=2 or 3\n",
        "    \"margin\",                                # ambiguous — expect higher k\n",
        "    \"can I withdraw money on the same day\",  # specific — expect k=2\n",
        "]\n",
        "\n",
        "for q in test_queries:\n",
        "    candidates = hybrid_search(q, stage1_k=25)\n",
        "    reranked   = rerank(q, candidates, final_k=8)\n",
        "    adaptive   = adaptive_k(reranked, min_k=2, max_k=8)\n",
        "\n",
        "    scores = [f\"{s:.2f}\" for _, s in reranked[:8]]\n",
        "    gaps   = [\n",
        "        f\"{reranked[i][1]-reranked[i+1][1]:.2f}\"\n",
        "        for i in range(len(reranked)-1) if i < 7\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n  Query: '{q}'\")\n",
        "    print(f\"  CE scores: [{', '.join(scores)}]\")\n",
        "    print(f\"  Gaps:      [{', '.join(gaps)}]\")\n",
        "    print(f\"  {Fore.GREEN}Adaptive-k = {len(adaptive)}{Style.RESET_ALL}  \"\n",
        "          f\"(Fixed-k=5 would use {min(5, len(reranked))} chunks)\")\n",
        "    for i, (chunk, score) in enumerate(adaptive, 1):\n",
        "        print(f\"    {i}. [CE={score:.3f}] {chunk.title}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "---\n",
        "## 7 · Generation with Grounded Citations\n",
        "\n",
        "We use `google/flan-t5-base` here — a small (250M param) instruction-tuned model that runs on CPU in Colab without any API key. It is not production-grade for generative quality, but it demonstrates the full pipeline architecture correctly.\n",
        "\n",
        "**To swap in a better model:** Replace the generation cell with an OpenAI/Anthropic API call — the retrieval pipeline above is completely model-agnostic. The prompt template stays identical.\n",
        "\n",
        "**Citation grounding:** Every answer cites the chunk IDs it used. If the model produces a claim not traceable to a retrieved chunk, that is your hallucination signal — catch it at evaluation time (Section 8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-generator",
      "metadata": {
        "id": "load-generator"
      },
      "outputs": [],
      "source": [
        "hdr(\"Loading generation model (Flan-T5-base — CPU, no API key)\")\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "GEN_MODEL     = \"google/flan-t5-base\"\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
        "gen_model     = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL)\n",
        "\n",
        "def generator(prompt: str, max_new_tokens: int = 256, **kwargs) -> list:\n",
        "    inputs  = gen_tokenizer(prompt, return_tensors=\"pt\",\n",
        "                            truncation=True, max_length=1024)\n",
        "    outputs = gen_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    text    = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return [{\"generated_text\": text}]\n",
        "\n",
        "ok(f\"Generator ready: {GEN_MODEL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rag-pipeline",
      "metadata": {
        "id": "rag-pipeline"
      },
      "outputs": [],
      "source": [
        "def build_prompt(query: str, chunks: List[Tuple[Chunk, float]]) -> str:\n",
        "    \"\"\"\n",
        "    Construct a grounded RAG prompt.\n",
        "    Each retrieved chunk is cited with a [SOURCE-N] tag.\n",
        "    The model is instructed to only use provided context.\n",
        "    \"\"\"\n",
        "    context_blocks = []\n",
        "    for i, (chunk, score) in enumerate(chunks, 1):\n",
        "        context_blocks.append(\n",
        "            f\"[SOURCE-{i}] {chunk.title}\\n{chunk.text[:600]}\"\n",
        "        )\n",
        "    context = \"\\n\\n\".join(context_blocks)\n",
        "\n",
        "    prompt = f\"\"\"Answer the following question using ONLY the provided sources.\n",
        "If the answer is not in the sources, say 'I could not find this in the Zerodha support docs.'\n",
        "Cite sources as [SOURCE-N] inline.\n",
        "\n",
        "SOURCES:\n",
        "{context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "ANSWER:\"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RAGResponse:\n",
        "    query:    str\n",
        "    answer:   str\n",
        "    sources:  List[Chunk]\n",
        "    latency_ms: float\n",
        "\n",
        "\n",
        "def rag_query(\n",
        "    query:      str,\n",
        "    stage1_k:   int = 25,\n",
        "    final_k:    int = 8,\n",
        "    use_adaptive_k: bool = True,\n",
        ") -> RAGResponse:\n",
        "    \"\"\"\n",
        "    Full production RAG pipeline:\n",
        "    Hybrid Search → Cross-Encoder Rerank → Adaptive-k → Generate\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Stage 1: Hybrid retrieval\n",
        "    candidates = hybrid_search(query, stage1_k=stage1_k)\n",
        "\n",
        "    # Stage 2: Cross-encoder reranking\n",
        "    reranked = rerank(query, candidates, final_k=final_k)\n",
        "\n",
        "    # Stage 3: Adaptive-k boundary detection\n",
        "    if use_adaptive_k:\n",
        "        final_chunks = adaptive_k(reranked, min_k=2, max_k=final_k)\n",
        "    else:\n",
        "        final_chunks = reranked[:5]\n",
        "\n",
        "    # Stage 4: Generate grounded answer\n",
        "    prompt = build_prompt(query, final_chunks)\n",
        "    output = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
        "\n",
        "    latency_ms = (time.time() - t0) * 1000\n",
        "\n",
        "    return RAGResponse(\n",
        "        query=query,\n",
        "        answer=output,\n",
        "        sources=[c for c, _ in final_chunks],\n",
        "        latency_ms=latency_ms,\n",
        "    )\n",
        "\n",
        "\n",
        "ok(\"RAG pipeline assembled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rag-demo",
      "metadata": {
        "id": "rag-demo"
      },
      "outputs": [],
      "source": [
        "hdr(\"Running RAG Pipeline — Live Queries\")\n",
        "\n",
        "demo_queries = [\n",
        "    \"How do I withdraw money from Zerodha to my bank account?\",\n",
        "    \"What is the difference between CNC and MIS orders?\",\n",
        "    \"Why was my F&O trade rejected due to insufficient margin?\",\n",
        "    \"How do I add a nominee to my Zerodha account?\",\n",
        "]\n",
        "\n",
        "responses: List[RAGResponse] = []\n",
        "\n",
        "for q in demo_queries:\n",
        "    print(f\"\\n{Fore.YELLOW}Q: {q}{Style.RESET_ALL}\")\n",
        "    resp = rag_query(q)\n",
        "    responses.append(resp)\n",
        "\n",
        "    print(f\"{Fore.GREEN}A: {resp.answer}{Style.RESET_ALL}\")\n",
        "    print(f\"   Latency: {resp.latency_ms:.0f}ms | Sources used: {len(resp.sources)}\")\n",
        "    for i, src in enumerate(resp.sources, 1):\n",
        "        print(f\"   [{i}] {src.title} → {src.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-8",
      "metadata": {
        "id": "section-8"
      },
      "source": [
        "---\n",
        "## 8 · Evaluation — Hit Rate, Faithfulness, Hallucination Detection\n",
        "\n",
        "**The gap most teams skip:** A RAG system that looks good in demos can still fail silently in production. Two failure modes from the deck:\n",
        "\n",
        "1. **Bad retrieval** — the right chunk was not retrieved at all. No amount of generation quality fixes this. Measure: Hit Rate @ k.\n",
        "2. **Bad generation** — the chunk was retrieved but the model hallucinated or ignored it. Measure: Faithfulness score.\n",
        "\n",
        "We use a **synthetic golden QA set** generated from the corpus itself — no human annotation required for a demo. In production, seed this with real user queries from your support ticket logs.\n",
        "\n",
        "**Faithfulness check:** A simple heuristic — does the answer contain n-grams that appear in the retrieved source? Not a replacement for LLM-as-judge, but deterministic and fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "golden-qa",
      "metadata": {
        "id": "golden-qa"
      },
      "outputs": [],
      "source": [
        "hdr(\"Building Synthetic Golden QA Set\")\n",
        "\n",
        "# Hand-crafted query → expected article title pairs\n",
        "# In production: derive from support ticket logs or LLM-generated QA pairs\n",
        "GOLDEN_QA = [\n",
        "    {\"query\": \"How do I withdraw money from Zerodha?\",\n",
        "     \"expected_titles\": [\"fund withdrawal\", \"withdraw\", \"payout\"]},\n",
        "\n",
        "    {\"query\": \"What is the difference between holdings and positions?\",\n",
        "     \"expected_titles\": [\"holdings and positions\", \"difference between holdings\"]},\n",
        "\n",
        "    {\"query\": \"What is MTF and how does it work?\",\n",
        "     \"expected_titles\": [\"mtf\", \"margin trading facility\"]},\n",
        "\n",
        "    {\"query\": \"What is F&O trading?\",\n",
        "     \"expected_titles\": [\"futures and options\", \"f&o\", \"derivatives\"]},\n",
        "\n",
        "    {\"query\": \"How to add a bank account in Zerodha?\",\n",
        "     \"expected_titles\": [\"add bank\", \"bank account\"]},\n",
        "\n",
        "    {\"query\": \"What are intraday margins?\",\n",
        "     \"expected_titles\": [\"intraday\", \"margin\", \"mis\"]},\n",
        "\n",
        "    {\"query\": \"How to create a Zerodha support ticket?\",\n",
        "     \"expected_titles\": [\"ticket\", \"create a ticket\", \"raise a ticket\"]},\n",
        "\n",
        "    {\"query\": \"What is a stop loss order?\",\n",
        "     \"expected_titles\": [\"stop loss\", \"sl order\"]},\n",
        "\n",
        "    {\"query\": \"How do I invest in mutual funds on Coin?\",\n",
        "     \"expected_titles\": [\"mutual fund\", \"coin\", \"invest\"]},\n",
        "\n",
        "    {\"query\": \"What is the settlement process for equity trades?\",\n",
        "     \"expected_titles\": [\"settlement\", \"t+1\", \"delivery\"]},\n",
        "]\n",
        "\n",
        "ok(f\"Golden QA set: {len(GOLDEN_QA)} query-answer pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eval-metrics",
      "metadata": {
        "id": "eval-metrics"
      },
      "outputs": [],
      "source": [
        "def hit_at_k(\n",
        "    retrieved_chunks: List[Chunk],\n",
        "    expected_title_fragments: List[str],\n",
        "    k: int = 5,\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if any of the top-k retrieved chunks match\n",
        "    at least one expected title fragment (case-insensitive substring).\n",
        "    \"\"\"\n",
        "    top_titles = [c.title.lower() for c in retrieved_chunks[:k]]\n",
        "    for frag in expected_title_fragments:\n",
        "        for title in top_titles:\n",
        "            if frag.lower() in title:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def faithfulness_score(answer: str, source_chunks: List[Chunk]) -> float:\n",
        "    \"\"\"\n",
        "    Heuristic faithfulness check.\n",
        "    Counts what fraction of answer trigrams appear in the source corpus.\n",
        "\n",
        "    Not a replacement for LLM-as-judge — but deterministic, fast, and\n",
        "    surprisingly effective at catching severe hallucinations.\n",
        "    \"\"\"\n",
        "    def ngrams(text: str, n: int = 3) -> set:\n",
        "        tokens = re.findall(r\"[a-z0-9]+\", text.lower())\n",
        "        return {tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)}\n",
        "\n",
        "    answer_ngrams = ngrams(answer)\n",
        "    if not answer_ngrams:\n",
        "        return 0.0\n",
        "\n",
        "    source_text = \" \".join(c.text for c in source_chunks)\n",
        "    source_ngrams = ngrams(source_text)\n",
        "\n",
        "    overlap = answer_ngrams & source_ngrams\n",
        "    return len(overlap) / len(answer_ngrams)\n",
        "\n",
        "\n",
        "ok(\"Evaluation functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "run-eval",
      "metadata": {
        "id": "run-eval"
      },
      "outputs": [],
      "source": [
        "hdr(\"Running Evaluation — Production RAG vs Naive Dense-Only\")\n",
        "\n",
        "# ── Production pipeline (Hybrid + Rerank + Adaptive-k) ────────────\n",
        "prod_results = []\n",
        "for qa in tqdm(GOLDEN_QA, desc=\"Production RAG\"):\n",
        "    candidates = hybrid_search(qa[\"query\"], stage1_k=25)\n",
        "    reranked   = rerank(qa[\"query\"], candidates, final_k=8)\n",
        "    final      = adaptive_k(reranked, min_k=2, max_k=8)\n",
        "    hit        = hit_at_k([c for c, _ in final], qa[\"expected_titles\"], k=5)\n",
        "    prompt     = build_prompt(qa[\"query\"], final)\n",
        "    answer     = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
        "    faith      = faithfulness_score(answer, [c for c, _ in final])\n",
        "    prod_results.append({\"hit\": hit, \"faithfulness\": faith,\n",
        "                         \"k_used\": len(final), \"answer\": answer})\n",
        "\n",
        "# ── Naive baseline (dense-only, fixed top-5) ──────────────────────\n",
        "naive_results = []\n",
        "for qa in tqdm(GOLDEN_QA, desc=\"Naive Dense-only\"):\n",
        "    dense_hits = dense_search(qa[\"query\"], top_k=5)\n",
        "    chunks     = [all_chunks[idx] for idx, _ in dense_hits]\n",
        "    hit        = hit_at_k(chunks, qa[\"expected_titles\"], k=5)\n",
        "    prompt     = build_prompt(qa[\"query\"], [(c, 0.0) for c in chunks])\n",
        "    answer     = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
        "    faith      = faithfulness_score(answer, chunks)\n",
        "    naive_results.append({\"hit\": hit, \"faithfulness\": faith,\n",
        "                          \"k_used\": 5, \"answer\": answer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eval-summary",
      "metadata": {
        "id": "eval-summary"
      },
      "outputs": [],
      "source": [
        "hdr(\"Evaluation Results\")\n",
        "\n",
        "prod_hit_rate = sum(r[\"hit\"] for r in prod_results) / len(prod_results)\n",
        "prod_faith    = sum(r[\"faithfulness\"] for r in prod_results) / len(prod_results)\n",
        "prod_avg_k    = sum(r[\"k_used\"] for r in prod_results) / len(prod_results)\n",
        "\n",
        "naive_hit_rate = sum(r[\"hit\"] for r in naive_results) / len(naive_results)\n",
        "naive_faith    = sum(r[\"faithfulness\"] for r in naive_results) / len(naive_results)\n",
        "\n",
        "print(f\"{'Metric':<35} {'Naive Dense-only':>18} {'Production RAG':>18}\")\n",
        "print(\"─\" * 73)\n",
        "print(f\"{'Hit Rate @ 5':<35} {naive_hit_rate:>17.1%} {prod_hit_rate:>17.1%}\")\n",
        "print(f\"{'Faithfulness (trigram overlap)':<35} {naive_faith:>17.2f} {prod_faith:>17.2f}\")\n",
        "print(f\"{'Avg chunks sent to LLM':<35} {'5 (fixed)':>18} {prod_avg_k:>17.1f}\")\n",
        "print(\"─\" * 73)\n",
        "print()\n",
        "\n",
        "# Per-query breakdown\n",
        "print(f\"{'Query':<52} {'Naive Hit':>11} {'Prod Hit':>10} {'Prod k':>8}\")\n",
        "print(\"─\" * 84)\n",
        "for i, qa in enumerate(GOLDEN_QA):\n",
        "    q = qa[\"query\"][:50]\n",
        "    nh = Fore.GREEN + \"✓\" + Style.RESET_ALL if naive_results[i][\"hit\"] else Fore.RED + \"✗\" + Style.RESET_ALL\n",
        "    ph = Fore.GREEN + \"✓\" + Style.RESET_ALL if prod_results[i][\"hit\"]  else Fore.RED + \"✗\" + Style.RESET_ALL\n",
        "    pk = prod_results[i][\"k_used\"]\n",
        "    print(f\"  {q:<60} {nh:>9} {ph:>19} {pk:>8}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-9",
      "metadata": {
        "id": "section-9"
      },
      "source": [
        "---\n",
        "## 9 · Interactive Query Interface\n",
        "\n",
        "Type any question about Zerodha below and the full production pipeline runs — Hybrid Search → Cross-Encoder Rerank → Adaptive-k → Grounded Generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interactive",
      "metadata": {
        "id": "interactive"
      },
      "outputs": [],
      "source": [
        "def ask(query: str, verbose: bool = True) -> RAGResponse:\n",
        "    \"\"\"\n",
        "    Ask any question. Full pipeline runs automatically.\n",
        "    Set verbose=False to suppress source details.\n",
        "    \"\"\"\n",
        "    resp = rag_query(query)\n",
        "\n",
        "    print(f\"\\n{Fore.YELLOW}{'─'*60}\")\n",
        "    print(f\"Q: {query}\")\n",
        "    print(f\"{'─'*60}{Style.RESET_ALL}\")\n",
        "    print(f\"\\n{Fore.GREEN}{resp.answer}{Style.RESET_ALL}\")\n",
        "    print(f\"\\n  Pipeline: Hybrid Search → CE Rerank → Adaptive-k={len(resp.sources)}\")\n",
        "    print(f\"  Latency:  {resp.latency_ms:.0f}ms\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n  {Fore.CYAN}Sources:{Style.RESET_ALL}\")\n",
        "        for i, src in enumerate(resp.sources, 1):\n",
        "            print(f\"    [{i}] {src.title}\")\n",
        "            print(f\"         {src.url}\")\n",
        "\n",
        "    return resp\n",
        "\n",
        "\n",
        "# ── Try it ──────────────────────────────────────────────────────\n",
        "# Modify this query to test any Zerodha support question:\n",
        "_ = ask(\"What documents do I need to open a Zerodha account?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "more-queries",
      "metadata": {
        "id": "more-queries"
      },
      "outputs": [],
      "source": [
        "# More queries — uncomment and run any of these\n",
        "# print(\"Uncomment any query below and run this cell.\")\n",
        "\n",
        "_ = ask(\"How long does it take for a fund withdrawal to reach my bank?\")\n",
        "# _ = ask(\"What happens to my F&O position on expiry day?\")\n",
        "# _ = ask(\"Can I trade in NRI account on Zerodha?\")\n",
        "# _ = ask(\"What is a GTT order and how do I place one?\")\n",
        "# _ = ask(\"What is the brokerage charged on intraday equity trades?\")\n",
        "# _ = ask(\"How to convert CNC position to MIS?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "closing-notes",
      "metadata": {
        "id": "closing-notes"
      },
      "source": [
        "---\n",
        "## Key Takeaways\n",
        "\n",
        "```\n",
        "Production RAG is not:\n",
        "  Chunk → Embed → top-k → Generate\n",
        "\n",
        "Production RAG is:\n",
        "  Semantic Chunk (sentence-aware, overlapping)\n",
        "  → Hybrid Retrieval (BM25 + Dense, fused with RRF)\n",
        "  → Cross-Encoder Rerank (precision pass on shortlist)\n",
        "  → Adaptive-k (score-gap boundary — no fixed k)\n",
        "  → Grounded Generation (citations, no hallucination budget)\n",
        "  → Continuous Eval (Hit Rate + Faithfulness per release)\n",
        "```\n",
        "\n",
        "**Swap out for production:**\n",
        "\n",
        "| This notebook | Production equivalent |\n",
        "|---------------|----------------------|\n",
        "| `QdrantClient(':memory:')` | Qdrant Cloud / self-hosted |\n",
        "| `flan-t5-base` | GPT-4o / Claude 3.5 / Gemini |\n",
        "| `all-MiniLM-L6-v2` | Domain-fine-tuned embedding model |\n",
        "| `BM25Okapi` in RAM | Elasticsearch sparse / Qdrant FastEmbed |\n",
        "| Synthetic QA eval | Real user query logs + human annotation |\n",
        "\n",
        "---\n",
        "*Indranil Chandra · ML & Data Architect, Upstox · Co-organiser, GDG MAD Mumbai*"
      ]
    }
  ]
}