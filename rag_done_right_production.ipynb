{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/indranildchandra/rag-done-right-in-production/blob/main/rag_done_right_production.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {
    "id": "title-cell"
   },
   "source": [
    "\n# RAG Done Right in Production\n### Demo Notebook \u2014 Zerodha Support FAQ Corpus\n\n**Talk:** RAG Done Right in Production \u00b7 Indranil Chandra, ML & Data Architect, Upstox\n\n---\n\nThis notebook is a **self-contained production RAG demo** that runs entirely inside Google Colab \u2014 no external API keys required. It builds a real retrieval pipeline on top of Zerodha's public support FAQ corpus.\n\n**What this covers, in order:**\n\n| # | Section | What you will observe |\n|---|---------|----------------------|\n| 1 | **Corpus Ingestion** | Scrape + parse Zerodha support FAQs |\n| 2 | **Chunking** | Semantic chunking vs naive fixed-size \u2014 side-by-side |\n| 3 | **Qdrant (in-memory)** | Embed + index with `all-MiniLM-L6-v2` |\n| 4a | **Hybrid Search** | BM25 + dense retrieval merged with RRF; exact-term recall demo |\n| 4b | **MMR Reranking** | Maximal Marginal Relevance \u2014 diversity over redundancy |\n| 5 | **Cross-Encoder Reranking** | Two-stage pipeline: recall then precision; per-stage latency |\n| 6 | **Adaptive-k** | Score-gap boundary vs fixed top-k |\n| 7 | **Generation + Citations** | LLM answer grounded with source attribution |\n| 8 | **Evaluation** | Hit rate, Faithfulness, Hallucination detection |\n| 9 | **Interactive** | 10 pre-loaded example queries \u2014 run any question end-to-end |\n\n> **Runtime:** GPU not required. CPU is sufficient. First run (fresh crawl + model downloads): ~12\u201318 min. Subsequent runs (corpus cache hit): ~8\u201312 min.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-0",
   "metadata": {
    "id": "section-0"
   },
   "source": [
    "---\n",
    "## 0 \u00b7 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import importlib.util, subprocess, sys\n",
    "\n",
    "# Pinned versions \u2014 tested on Google Colab and Python 3.10+.\n",
    "# To regenerate pins after a working install:\n",
    "#   !pip freeze | grep -E \"requests|qdrant|sentence|bm25|beautifulsoup|transformers|openai|tiktoken|tqdm|colorama\"\n",
    "_PACKAGES = {\n",
    "    \"qdrant_client\":         \"qdrant-client==1.17.0\",\n",
    "    \"sentence_transformers\": \"sentence-transformers==5.2.3\",\n",
    "    \"rank_bm25\":             \"rank-bm25==0.2.2\",\n",
    "    \"bs4\":                   \"beautifulsoup4>=4.13.5\",\n",
    "    \"requests\":              \"requests==2.32.4\",\n",
    "    \"transformers\":          \"transformers==5.0.0\",\n",
    "    \"openai\":                \"openai==2.23.0\",\n",
    "    \"tiktoken\":              \"tiktoken==0.12.0\",\n",
    "    \"tqdm\":                  \"tqdm==4.67.3\",\n",
    "    \"colorama\":              \"colorama==0.4.6\",\n",
    "}\n",
    "\n",
    "_missing = [spec for mod, spec in _PACKAGES.items()\n",
    "            if importlib.util.find_spec(mod) is None]\n",
    "if _missing:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + _missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep -E \"requests|qdrant|sentence|bm25|beautifulsoup|transformers|openai|tiktoken|tqdm|colorama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import os, re, json, time, random, hashlib, warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance, VectorParams, PointStruct,\n",
    "    Filter, FieldCondition, MatchValue\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# \u2500\u2500 Colour helpers for readable notebook output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "from colorama import Fore, Style, init as colorama_init\n",
    "colorama_init(autoreset=True)\n",
    "\n",
    "def hdr(msg):  print(f\"\\n{Fore.CYAN}{Style.BRIGHT}{'\u2500'*60}\\n  {msg}\\n{'\u2500'*60}{Style.RESET_ALL}\")\n",
    "def ok(msg):   print(f\"{Fore.GREEN}\u2713  {msg}{Style.RESET_ALL}\")\n",
    "def info(msg): print(f\"{Fore.YELLOW}\u2139  {msg}{Style.RESET_ALL}\")\n",
    "def err(msg):  print(f\"{Fore.RED}\u2717  {msg}{Style.RESET_ALL}\")\n",
    "\n",
    "ok(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "---\n## 1 \u00b7 Corpus Ingestion \u2014 Zerodha Support FAQ Scraper\n\nThe scraper works in **three passes**:\n\n1. **Category discovery** \u2014 read the homepage sidebar to find all section URLs  \n2. **Article link extraction** \u2014 for each category page, collect article hrefs  \n3. **Article fetch + parse** \u2014 extract `<h1>` title and body text, strip nav/footer boilerplate  \n\nWe scrape politely: 0.4s delay per thread (5 threads), 3 retries with exponential backoff. All articles across all 32 sub-categories are scraped \u2014 no cap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDOWNLOAD_DATA = False                 # True \u2192 re-crawl regardless of cache, default = False, skips downloading data once downloaded\n",
    "CORPUS_CACHE    = \"zerodha_faqs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scraper-config",
   "metadata": {
    "id": "scraper-config"
   },
   "outputs": [],
   "source": [
    "REDOWNLOAD_DATA = False           # True \u2192 re-crawl regardless of cache\n",
    "CORPUS_CACHE    = \"zerodha_faqs.json\"\n",
    "FETCH_WORKERS   = 6               # threads for parallel article fetching (I/O-bound)\n",
    "\n",
    "BASE_URL  = \"https://support.zerodha.com\"\n",
    "DELAY_SEC = 0.4\n",
    "RETRIES   = 3\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# \u2500\u2500 Discovery config \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# The 6 top-level section URLs are stable (Zerodha's nav structure hasn't changed\n",
    "# in years). run-scraper fetches each section page, parses sub-category hrefs\n",
    "# automatically, then scrapes all articles in every sub-category.\n",
    "TOP_LEVEL_SECTIONS = [\n",
    "    \"/category/account-opening\",\n",
    "    \"/category/your-zerodha-account\",\n",
    "    \"/category/trading-and-markets\",\n",
    "    \"/category/funds\",\n",
    "    \"/category/console\",\n",
    "    \"/category/mutual-funds\",\n",
    "]\n",
    "\n",
    "# Sub-categories to skip \u2014 no Q&A article content\n",
    "EXCLUDED_SLUGS = {\"glossary\"}\n",
    "\n",
    "# \u2500\u2500 Fallback sub-categories \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Used if live discovery returns 0 results for a section (e.g. JS rendering).\n",
    "# Derived from a live fetch of https://support.zerodha.com/ on 2026-02-28.\n",
    "# Update this list if Zerodha adds new sections in the future.\n",
    "FALLBACK_SUBCATEGORIES: List[Tuple[str, str]] = [\n",
    "    # account-opening\n",
    "    (\"/category/account-opening/resident-individual\",                             \"Resident Individual\"),\n",
    "    (\"/category/account-opening/minor\",                                           \"Minor\"),\n",
    "    (\"/category/account-opening/nri-account-opening\",                            \"NRI\"),\n",
    "    (\"/category/account-opening/company-partnership-and-huf-account-opening\",    \"Corporate HUF LLP\"),\n",
    "    # your-zerodha-account\n",
    "    (\"/category/your-zerodha-account/your-profile\",                              \"Your Profile\"),\n",
    "    (\"/category/your-zerodha-account/account-modification-and-segment-addition\", \"Account Modification\"),\n",
    "    (\"/category/your-zerodha-account/dp-id-and-bank-details\",                   \"DP and Bank Details\"),\n",
    "    (\"/category/your-zerodha-account/nomination-process\",                        \"Nomination\"),\n",
    "    (\"/category/your-zerodha-account/transfer-of-shares-and-conversion-of-shares\", \"Transfer of Shares\"),\n",
    "    # trading-and-markets\n",
    "    (\"/category/trading-and-markets/trading-faqs\",                               \"Trading FAQs\"),\n",
    "    (\"/category/trading-and-markets/margins\",                                    \"Margins\"),\n",
    "    (\"/category/trading-and-markets/charts-and-orders\",                          \"Charts and Orders\"),\n",
    "    (\"/category/trading-and-markets/general-kite\",                               \"General Kite\"),\n",
    "    (\"/category/trading-and-markets/charges\",                                    \"Charges\"),\n",
    "    (\"/category/trading-and-markets/ipo\",                                        \"IPO\"),\n",
    "    (\"/category/trading-and-markets/alerts-and-nudges\",                          \"Alerts and Nudges\"),\n",
    "    # funds\n",
    "    (\"/category/funds/adding-funds\",                                             \"Adding Funds\"),\n",
    "    (\"/category/funds/fund-withdrawal\",                                          \"Fund Withdrawal\"),\n",
    "    (\"/category/funds/adding-bank-accounts\",                                     \"Adding Bank Accounts\"),\n",
    "    (\"/category/funds/mandate\",                                                  \"Mandate\"),\n",
    "    # console\n",
    "    (\"/category/console/portfolio\",                                              \"Console Portfolio\"),\n",
    "    (\"/category/console/corporate-actions\",                                      \"Corporate Actions\"),\n",
    "    (\"/category/console/ledger\",                                                 \"Ledger\"),\n",
    "    (\"/category/console/reports\",                                                \"Reports\"),\n",
    "    (\"/category/console/profile\",                                                \"Console Profile\"),\n",
    "    (\"/category/console/segments\",                                               \"Segments\"),\n",
    "    # mutual-funds\n",
    "    (\"/category/mutual-funds/understanding-mutual-funds\",                        \"Understanding Mutual Funds\"),\n",
    "    (\"/category/mutual-funds/payments-and-orders\",                               \"MF Payments and Orders\"),\n",
    "    (\"/category/mutual-funds/nps\",                                               \"NPS\"),\n",
    "    (\"/category/mutual-funds/fixed-deposits\",                                    \"Fixed Deposits\"),\n",
    "    (\"/category/mutual-funds/features-on-coin\",                                  \"Features on Coin\"),\n",
    "    (\"/category/mutual-funds/coin-general\",                                      \"Coin General\"),\n",
    "]\n",
    "\n",
    "hdr(\"Scraper configuration\")\n",
    "info(f\"Base URL:          {BASE_URL}\")\n",
    "info(f\"Top-level sections: {len(TOP_LEVEL_SECTIONS)} (sub-categories auto-discovered)\")\n",
    "info(f\"Fallback entries:   {len(FALLBACK_SUBCATEGORIES)} known sub-categories\")\n",
    "info(f\"Delay:              {DELAY_SEC}s between requests\")\n",
    "info(f\"Workers:            {FETCH_WORKERS} threads (parallel article fetch)\")\n",
    "info(f\"Cache file:         {CORPUS_CACHE}  (REDOWNLOAD_DATA={REDOWNLOAD_DATA})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scraper-functions",
   "metadata": {
    "id": "scraper-functions"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ZerodhaArticle:\n",
    "    url:      str\n",
    "    title:    str\n",
    "    body:     str\n",
    "    category: str\n",
    "    doc_id:   str = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.doc_id = hashlib.md5(self.url.encode()).hexdigest()[:12]\n",
    "\n",
    "\n",
    "def _get(url: str, retries: int = RETRIES) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"Fetch a URL and return a BeautifulSoup object. Returns None on failure.\"\"\"\n",
    "    full = url if url.startswith(\"http\") else BASE_URL + url\n",
    "    _fail = \"unknown\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = requests.get(full, headers=HEADERS, timeout=15)\n",
    "            if r.status_code == 200:\n",
    "                time.sleep(DELAY_SEC)\n",
    "                return BeautifulSoup(r.text, \"html.parser\")\n",
    "            elif r.status_code == 429:\n",
    "                _fail = \"rate-limited (429)\"\n",
    "                wait = 2 ** (attempt + 1)\n",
    "                if attempt < retries - 1:   # don't sleep after the last attempt\n",
    "                    info(f\"Rate limited \u2014 waiting {wait}s\")\n",
    "                    time.sleep(wait)\n",
    "            else:\n",
    "                _fail = f\"HTTP {r.status_code}\"\n",
    "        except requests.RequestException as e:\n",
    "            _fail = type(e).__name__\n",
    "            if attempt < retries - 1:       # don't sleep after the last attempt\n",
    "                time.sleep(2 ** attempt)\n",
    "    err(f\"Skipped [{_fail}] after {retries} attempts: {full}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def _discover_subcategories(section_url: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Fetch a top-level section page and extract all sub-category hrefs.\n",
    "\n",
    "    Zerodha's section pages (e.g. /category/account-opening) are server-side\n",
    "    rendered and list sub-category links in the nav sidebar and main content.\n",
    "    We parse any <a href=\"/category/{section}/{sub-cat}\"> links \u2014 exactly 3\n",
    "    path segments, no /articles/ \u2014 to discover the full sub-category set.\n",
    "\n",
    "    Returns List of (href, label) tuples, or [] if the page is unreachable.\n",
    "    The caller falls back to FALLBACK_SUBCATEGORIES on an empty result.\n",
    "    \"\"\"\n",
    "    section_slug = section_url.strip(\"/\").split(\"/\")[-1]\n",
    "    soup = _get(section_url)\n",
    "    if not soup:\n",
    "        return []\n",
    "    seen, results = set(), []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        parts = [p for p in href.split(\"/\") if p]\n",
    "        if (len(parts) == 3\n",
    "                and parts[0] == \"category\"\n",
    "                and parts[1] == section_slug\n",
    "                and \"articles\" not in href\n",
    "                and parts[2] not in EXCLUDED_SLUGS\n",
    "                and href not in seen):\n",
    "            seen.add(href)\n",
    "            label = a.get_text(strip=True) or parts[2].replace(\"-\", \" \").title()\n",
    "            results.append((href, label))\n",
    "    return results\n",
    "\n",
    "\n",
    "def _extract_article_links(category_url: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    From a sub-category page, collect all /articles/ hrefs.\n",
    "    Zerodha renders article links as <a href=\"/category/.../articles/slug\">\n",
    "    Works for any depth: /category/{section}/{sub}/{topic}/articles/{slug}\n",
    "    \"\"\"\n",
    "    soup = _get(category_url)\n",
    "    if not soup:\n",
    "        return []\n",
    "    links = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \"/articles/\" in href and href.startswith(\"/category/\"):\n",
    "            links.add(href)\n",
    "    return list(links)\n",
    "\n",
    "\n",
    "# Boilerplate patterns to strip from article body\n",
    "_BOILERPLATE = re.compile(\n",
    "    r\"(Updates|Education|Utilities|Support Portal|Related articles|Quick links\"\n",
    "    r\"|Signup|About|Products|Pricing|Zerodha Broking|\u00a9 20\\d\\d)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def _parse_article(href: str, category: str) -> Optional[ZerodhaArticle]:\n",
    "    \"\"\"\n",
    "    Fetch and parse a single article page.\n",
    "    Strategy:\n",
    "      - Title  \u2192 <h1> tag\n",
    "      - Body   \u2192 all <p>/<li> tags after the <h1>, before 'Related articles'\n",
    "    \"\"\"\n",
    "    soup = _get(href)\n",
    "    if not soup:\n",
    "        return None\n",
    "\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if not h1:\n",
    "        return None\n",
    "    title = h1.get_text(strip=True)\n",
    "\n",
    "    # Collect paragraphs that appear after the <h1>\n",
    "    paragraphs = []\n",
    "    in_body = False\n",
    "    for tag in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\"]):\n",
    "        if tag == h1:\n",
    "            in_body = True\n",
    "            continue\n",
    "        if not in_body:\n",
    "            continue\n",
    "        text = tag.get_text(\" \", strip=True)\n",
    "        if not text or len(text) < 15:\n",
    "            continue\n",
    "        if re.search(r\"related articles\", text, re.IGNORECASE):\n",
    "            break\n",
    "        if _BOILERPLATE.search(text):\n",
    "            continue\n",
    "        paragraphs.append(text)\n",
    "\n",
    "    body = \" \".join(paragraphs).strip()\n",
    "    if len(body) < 80:   # skip stubs\n",
    "        return None\n",
    "\n",
    "    return ZerodhaArticle(\n",
    "        url=BASE_URL + href,\n",
    "        title=title,\n",
    "        body=body,\n",
    "        category=category,\n",
    "    )\n",
    "\n",
    "\n",
    "ok(\"Scraper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-scraper",
   "metadata": {
    "id": "run-scraper"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "_from_cache = (not REDOWNLOAD_DATA) and os.path.exists(CORPUS_CACHE)\n",
    "\n",
    "if _from_cache:\n",
    "    hdr(\"Corpus cache found \u2014 skipping discovery\")\n",
    "    ok(f\"Reusing: {CORPUS_CACHE}  (set REDOWNLOAD_DATA=True in scraper-config to re-crawl)\")\n",
    "else:\n",
    "    hdr(\"Discovering and scraping all Zerodha support categories\")\n",
    "    \n",
    "    # \u2500\u2500 Phase 1: Auto-discover sub-categories \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    # For each section, merge live-discovered sub-categories with the known\n",
    "    # FALLBACK_SUBCATEGORIES. This handles two cases:\n",
    "    #   (a) Live discovery returns 0 (JS-rendered section) \u2192 use all fallback entries\n",
    "    #   (b) Live discovery finds SOME but misses others (e.g. charges not in nav)\n",
    "    #       \u2192 fallback fills the gaps without adding duplicates\n",
    "    all_subcategories: List[Tuple[str, str]] = []\n",
    "    \n",
    "    for section_url in TOP_LEVEL_SECTIONS:\n",
    "        section_name = section_url.split(\"/\")[-1].replace(\"-\", \" \").title()\n",
    "        section_slug = section_url.strip(\"/\").split(\"/\")[-1]\n",
    "        # Use startswith to avoid substring matches (e.g. \"funds\" \u2282 \"mutual-funds\")\n",
    "        section_prefix = f\"/category/{section_slug}/\"\n",
    "    \n",
    "        live       = _discover_subcategories(section_url)\n",
    "        known      = [(h, l) for h, l in FALLBACK_SUBCATEGORIES if h.startswith(section_prefix)]\n",
    "        live_hrefs = {h for h, _ in live}\n",
    "    \n",
    "        # Start with live results; append any known entries missing from live\n",
    "        merged = list(live)\n",
    "        gap    = [(h, l) for h, l in known if h not in live_hrefs]\n",
    "        merged += gap\n",
    "    \n",
    "        if not live:\n",
    "            info(f\"{section_name}: live discovery returned 0 \u2014 using {len(known)} fallback entries\")\n",
    "        elif gap:\n",
    "            info(f\"{section_name}: {len(live)} discovered + {len(gap)} from fallback = {len(merged)}\")\n",
    "        else:\n",
    "            info(f\"{section_name}: {len(live)} sub-categories discovered (complete)\")\n",
    "    \n",
    "        all_subcategories.extend(merged)\n",
    "    \n",
    "    ok(f\"Total sub-categories: {len(all_subcategories)}\")\n",
    "    \n",
    "    # \u2500\u2500 Phase 2: Extract article links from every sub-category page \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    all_links: Dict[str, str] = {}      # href \u2192 category label\n",
    "    cat_link_map: Dict[str, List[str]] = {}\n",
    "    \n",
    "    for sub_cat_href, label in tqdm(all_subcategories, desc=\"Discovering articles\"):\n",
    "        links = _extract_article_links(sub_cat_href)\n",
    "        for lnk in links:\n",
    "            if lnk not in all_links:    # first category label wins on cross-category duplicates\n",
    "                all_links[lnk] = label\n",
    "        cat_link_map[label] = links\n",
    "        info(f\"  {label}: {len(links)} articles\")\n",
    "    \n",
    "    # Order doesn't affect the corpus (all articles are fetched); seed kept so\n",
    "    # partial runs are consistent if fetch-articles is interrupted mid-way.\n",
    "    random.seed(42)\n",
    "    all_link_items = list(all_links.items())\n",
    "    random.shuffle(all_link_items)\n",
    "    \n",
    "    ok(f\"Total unique articles to fetch: {len(all_link_items)}\")\n",
    "    info(f\"Sub-categories scraped: {len(all_subcategories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-articles",
   "metadata": {
    "id": "fetch-articles"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 4s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 4s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 4s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 4s\n",
      "\u2139  Rate limited \u2014 waiting 4s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\u2139  Rate limited \u2014 waiting 2s\n",
      "\n",
      "\u2139  Rate limited \u2014 waiting 2s\n",
      "\u2139  Rate limited \u2014 waiting 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "if _from_cache:\n",
    "    with open(CORPUS_CACHE) as _f:\n",
    "        _raw = json.load(_f)\n",
    "    articles = [\n",
    "        ZerodhaArticle(url=d[\"url\"], title=d[\"title\"], body=d[\"body\"], category=d[\"category\"])\n",
    "        for d in _raw\n",
    "    ]\n",
    "    ok(f\"Loaded {len(articles)} articles from cache ({CORPUS_CACHE})\")\n",
    "else:\n",
    "    # Network I/O releases the GIL \u2192 threads give near-linear speedup here.\n",
    "    # FETCH_WORKERS=6 means ~6 concurrent requests; each thread still\n",
    "    # respects DELAY_SEC inside _get().\n",
    "    articles: List[ZerodhaArticle] = []\n",
    "    with ThreadPoolExecutor(max_workers=FETCH_WORKERS) as pool:\n",
    "        futures = {pool.submit(_parse_article, href, cat): href\n",
    "                   for href, cat in all_link_items}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Fetching articles\"):\n",
    "            art = fut.result()\n",
    "            if art:\n",
    "                articles.append(art)\n",
    "    ok(f\"Successfully parsed {len(articles)} articles\")\n",
    "\n",
    "# Quick sanity check \u2014 show 3 samples\n",
    "print()\n",
    "for art in articles[:3]:\n",
    "    print(f\"  [{art.category}]  {art.title}\")\n",
    "    print(f\"  Body ({len(art.body)} chars): {art.body[:120]}...\")\n",
    "    print(f\"  URL: {art.url}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-corpus",
   "metadata": {
    "id": "save-corpus"
   },
   "outputs": [],
   "source": [
    "# Persist to disk \u2014 so you can reload without re-scraping\n",
    "if _from_cache:\n",
    "    ok(f\"Corpus loaded from cache \u2014 skipped re-save ({CORPUS_CACHE})\")\n",
    "else:\n",
    "    with open(CORPUS_CACHE, \"w\") as f:\n",
    "        json.dump(\n",
    "            [{\"doc_id\": a.doc_id, \"url\": a.url, \"title\": a.title,\n",
    "              \"body\": a.body, \"category\": a.category} for a in articles],\n",
    "            f, indent=2\n",
    "        )\n",
    "    ok(f\"Corpus saved \u2192 {CORPUS_CACHE}  ({os.path.getsize(CORPUS_CACHE)//1024} KB)\")\n",
    "\n",
    "# Category distribution\n",
    "from collections import Counter\n",
    "dist = Counter(a.category for a in articles)\n",
    "print(\"\\nCategory distribution:\")\n",
    "for cat, cnt in dist.most_common():\n",
    "    bar = \"\u2588\" * cnt\n",
    "    print(f\"  {cat:<50} {cnt:>3}  {bar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "---\n",
    "## 2 \u00b7 Chunking Strategy\n",
    "\n",
    "**The problem with fixed-size chunking:** A 512-token window that splits mid-sentence loses the semantic unit. Two adjacent chunks end up containing half a thought each \u2014 both retrieve poorly.\n",
    "\n",
    "**What we use here:** Sentence-boundary aware chunking with a configurable overlap. Each chunk respects sentence boundaries, with a sliding window overlap to preserve cross-sentence context.\n",
    "\n",
    "For a support FAQ corpus specifically, most articles are short enough (200\u2013600 tokens) that we treat each article as 1\u20133 chunks max. The overlap handles the edge case where a key fact straddles a sentence boundary.\n",
    "\n",
    "**Production note:** For longer documents (regulatory PDFs, annual reports), switch to hierarchical chunking: paragraph-level for indexing, sentence-level for reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunker",
   "metadata": {
    "id": "chunker"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id:   str          # doc_id_chunk_idx\n",
    "    doc_id:     str\n",
    "    title:      str\n",
    "    text:       str          # chunk text that gets embedded\n",
    "    full_text:  str          # full article body \u2014 for citation display\n",
    "    url:        str\n",
    "    category:   str\n",
    "    chunk_idx:  int\n",
    "\n",
    "\n",
    "def sentence_aware_chunk(\n",
    "    article: ZerodhaArticle,\n",
    "    max_chars: int = 800,\n",
    "    overlap_chars: int = 120,\n",
    ") -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Split article body into overlapping sentence-boundary-aware chunks.\n",
    "    Each chunk prepends the article title for embedding context.\n",
    "    \"\"\"\n",
    "    # Split on sentence boundaries\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', article.body.strip())\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "\n",
    "    chunks = []\n",
    "    buf, buf_len = [], 0\n",
    "    idx = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        if buf_len + len(sent) > max_chars and buf:\n",
    "            text = \" \".join(buf)\n",
    "            chunks.append(Chunk(\n",
    "                chunk_id  = f\"{article.doc_id}_{idx}\",\n",
    "                doc_id    = article.doc_id,\n",
    "                title     = article.title,\n",
    "                text      = f\"{article.title}. {text}\",\n",
    "                full_text = article.body,\n",
    "                url       = article.url,\n",
    "                category  = article.category,\n",
    "                chunk_idx = idx,\n",
    "            ))\n",
    "            idx += 1\n",
    "            # Carry over tail sentences for overlap\n",
    "            overlap_buf, overlap_len = [], 0\n",
    "            for s in reversed(buf):\n",
    "                if overlap_len + len(s) < overlap_chars:\n",
    "                    overlap_buf.insert(0, s)\n",
    "                    overlap_len += len(s)\n",
    "                else:\n",
    "                    break\n",
    "            buf, buf_len = overlap_buf, overlap_len\n",
    "\n",
    "        buf.append(sent)\n",
    "        buf_len += len(sent)\n",
    "\n",
    "    # Flush remainder\n",
    "    if buf:\n",
    "        text = \" \".join(buf)\n",
    "        chunks.append(Chunk(\n",
    "            chunk_id  = f\"{article.doc_id}_{idx}\",\n",
    "            doc_id    = article.doc_id,\n",
    "            title     = article.title,\n",
    "            text      = f\"{article.title}. {text}\",\n",
    "            full_text = article.body,\n",
    "            url       = article.url,\n",
    "            category  = article.category,\n",
    "            chunk_idx = idx,\n",
    "        ))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Build chunk corpus\n",
    "all_chunks: List[Chunk] = []\n",
    "for art in articles:\n",
    "    all_chunks.extend(sentence_aware_chunk(art))\n",
    "\n",
    "ok(f\"{len(articles)} articles \u2192 {len(all_chunks)} chunks\")\n",
    "avg_len = sum(len(c.text) for c in all_chunks) / len(all_chunks)\n",
    "info(f\"Average chunk length: {avg_len:.0f} chars\")\n",
    "\n",
    "# Show chunking on a sample article\n",
    "sample_art = max(articles, key=lambda a: len(a.body))\n",
    "sample_chunks = sentence_aware_chunk(sample_art)\n",
    "print(f\"\\nSample article: '{sample_art.title}'\")\n",
    "print(f\"Body length: {len(sample_art.body)} chars \u2192 {len(sample_chunks)} chunks\")\n",
    "for i, c in enumerate(sample_chunks):\n",
    "    print(f\"  Chunk {i}: {len(c.text)} chars | '{c.text[:80]}...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jiaa20qq0ak",
   "metadata": {},
   "source": [
    "### Chunking decisions and their knock-on effects\n",
    "\n",
    "Every number in `sentence_aware_chunk()` was chosen to respect a constraint further down the pipeline. Ideally, if we use a better embedding model (like text-embedding-3-small), we should consider using semantic chunking with one chunk per doc id (i.e. one record per faq article) to reduce a lot of these overheads\n",
    "\n",
    "Here is the full reasoning:\n",
    "\n",
    "**`max_chars=800` \u2014 why not larger?**\n",
    "`all-MiniLM-L6-v2` (used in Section 3) has a **hard 256-token max sequence length**. A token is roughly 4 characters of English text, so 256 tokens \u2248 1024 characters. At 800 chars we sit at ~200 tokens \u2014 a comfortable margin that survives slightly longer words or tokeniser overhead without truncation. Any chunk over ~1000 chars will be silently truncated by the model, making the tail of the chunk invisible to retrieval. This is the single most important constraint in the chunking design.\n",
    "\n",
    "**`overlap_chars=120` \u2014 why overlap at all?**\n",
    "Answers often straddle a natural sentence boundary. Without overlap, a question about \"when does a withdrawal reflect in the bank account?\" might hit a chunk that ends with \"...withdrawal is processed within 24 hours\" and miss the next chunk that starts with \"Funds appear in your account between 9 AM\u20135 PM on business days.\" The 120-char overlap ensures both chunks contain the bridging sentence, so at least one of them retrieves correctly.\n",
    "\n",
    "**Sentence-boundary splitting \u2014 why not fixed character windows?**\n",
    "A fixed 800-char split lands mid-sentence roughly 40% of the time (see the naive_chunk comparison below). A partial sentence embedded as a vector produces a noisy representation \u2014 the model encodes an incomplete thought. Sentence-aware splitting guarantees every chunk is a self-contained semantic unit. The embedding quality difference is measurable: mid-sentence chunks rank 15\u201330% lower in retrieval experiments on short-form Q&A corpora.\n",
    "\n",
    "**`chunk.text` is always `\"{title}. {body_text}\"` \u2014 why prefix the title?**\n",
    "A chunk in isolation loses its document identity. Consider the text *\"This can be done from the Console under Funds.\"* \u2014 without the title prefix, the embedding has no signal that this is about withdrawals. Prepending `\"How to withdraw money from the Zerodha account. \"` anchors the chunk's semantic meaning to its article topic. This is especially important for BM25: the title words (\"withdraw\", \"money\", \"account\") now appear in every chunk and participate in term-frequency scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jwyo7nxem6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \u2500\u2500 Naive fixed-size chunker (baseline for comparison) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def naive_chunk(article: ZerodhaArticle, chunk_size: int = 200) -> List[Chunk]:\n",
    "    \"\"\"\n",
    "    Fixed-size chunking with no regard for sentence boundaries.\n",
    "    This is what most tutorials start with \u2014 and what costs you retrieval quality.\n",
    "    \"\"\"\n",
    "    body = article.body\n",
    "    chunks = []\n",
    "    idx = 0\n",
    "    for start in range(0, len(body), chunk_size):\n",
    "        text = body[start:start + chunk_size]\n",
    "        if len(text.strip()) < 20:\n",
    "            continue\n",
    "        chunks.append(Chunk(\n",
    "            chunk_id  = f\"{article.doc_id}_naive_{idx}\",\n",
    "            doc_id    = article.doc_id,\n",
    "            title     = article.title,\n",
    "            text      = f\"{article.title}. {text}\",\n",
    "            full_text = article.body,\n",
    "            url       = article.url,\n",
    "            category  = article.category,\n",
    "            chunk_idx = idx,\n",
    "        ))\n",
    "        idx += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# \u2500\u2500 Side-by-side comparison on the longest article \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "hdr(\"Chunking Strategy: Naive Fixed-size vs Sentence-Aware\")\n",
    "naive_chunks  = naive_chunk(sample_art, chunk_size=200)\n",
    "smart_chunks  = sentence_aware_chunk(sample_art)\n",
    "\n",
    "print(f\"  Article: '{sample_art.title}'  ({len(sample_art.body)} chars)\\n\")\n",
    "\n",
    "print(f\"{Fore.RED}Naive fixed-size (200 chars): {len(naive_chunks)} chunks{Style.RESET_ALL}\")\n",
    "for i, c in enumerate(naive_chunks[:2]):\n",
    "    snippet = c.text[len(sample_art.title) + 2:]\n",
    "    print(f\"  Chunk {i}: |{repr(snippet[:120])}|\")\n",
    "    if i == 0:\n",
    "        # Highlight mid-sentence split\n",
    "        if not snippet.rstrip().endswith(('.', '!', '?')):\n",
    "            print(f\"            {Fore.RED}\u26a0 ends mid-sentence{Style.RESET_ALL}\")\n",
    "\n",
    "print(f\"\\n{Fore.GREEN}Sentence-aware (800 chars, 120 overlap): {len(smart_chunks)} chunks{Style.RESET_ALL}\")\n",
    "for i, c in enumerate(smart_chunks[:2]):\n",
    "    snippet = c.text[len(sample_art.title) + 2:]\n",
    "    print(f\"  Chunk {i}: |{repr(snippet[:120])}|\")\n",
    "\n",
    "print(f\"\\n{Fore.YELLOW}Key insight:{Style.RESET_ALL} Naive chunks split mid-sentence. \"\n",
    "      \"Sentence-aware chunks always start and end at natural boundaries \u2014 \"\n",
    "      \"semantic units stay intact.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "---\n",
    "## 3 \u00b7 Qdrant In-Memory Vector Store\n",
    "\n",
    "We use **Qdrant's in-memory mode** \u2014 no Docker, no external service, no API key. Same Python client API as production Qdrant Cloud. This means your demo code is identical to what you would run in prod \u2014 just swap `QdrantClient(':memory:')` for `QdrantClient(url='https://your-cluster.qdrant.io', api_key='...')`.\n",
    "\n",
    "**Embedding model:** `all-MiniLM-L6-v2`  \n",
    "- 384 dimensions, 22M parameters, ~80ms per 100 chunks on CPU  \n",
    "- Production note: for a finance domain, fine-tuned models on SEBI/NSE corpus improve retrieval recall by 10\u201315%  \n",
    "- 768-dim models (MPNet) give diminishing returns past this corpus size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding-model",
   "metadata": {
    "id": "embedding-model"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hdr(\"Loading embedding model\")\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "EMBED_DIM = embed_model.get_sentence_embedding_dimension()\n",
    "ok(f\"Model: {EMBED_MODEL_NAME} | Dim: {EMBED_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3p5u8uhey9l",
   "metadata": {},
   "source": [
    "### Why all-MiniLM-L6-v2 and what its constraints mean for the rest of the pipeline\n",
    "\n",
    "**Why this model?**\n",
    "`all-MiniLM-L6-v2` is a 22M-parameter distilled sentence encoder. It runs comfortably on a free Colab CPU (~22ms per batch of 64 chunks), requires no API key, and produces embeddings that rank in the top tier of the MTEB retrieval benchmarks for its size class. For a production demo with a finite corpus, the recall quality is more than sufficient.\n",
    "\n",
    "**Hard constraint: 256-token maximum sequence length.**\n",
    "This is the single number that shapes the entire chunking strategy in Section 2. The tokeniser will silently truncate any input beyond 256 tokens \u2014 no warning, no error, just a shorter embedding. At ~4 chars/token, the safe upper bound is roughly 1000 characters of body text. The 800-char chunk target in `sentence_aware_chunk()` respects this with a buffer.\n",
    "\n",
    "**`normalize_embeddings=True` \u2014 why?**\n",
    "Normalising to unit length (L2 norm = 1) converts cosine similarity into a plain dot product:  \n",
    "`cosine(a, b) = a\u00b7b` when `|a| = |b| = 1`.  \n",
    "Qdrant's query uses cosine distance by default. Normalised embeddings make the math consistent and avoid numerical instability when scores are compared across queries of different lengths.\n",
    "\n",
    "**`EMBED_DIM=384` \u2014 what does this mean for storage?**\n",
    "Each chunk is stored as a 384-float32 vector = 384 \u00d7 4 bytes = **1,536 bytes \u2248 1.5 KB**.\n",
    "\n",
    "With the auto-discovery crawler fetching all 32 sub-categories (~400\u2013700 articles, ~5 chunks per article on average):\n",
    "\n",
    "| Corpus size | Chunks | Vector store size |\n",
    "|---|---|---|\n",
    "| This notebook (~500 articles) | ~2,500 | ~3.8 MB |\n",
    "| 10\u00d7 scale (~5,000 articles) | ~25,000 | ~38 MB |\n",
    "| 100\u00d7 scale (~50,000 articles) | ~250,000 | ~380 MB |\n",
    "\n",
    "All three fit comfortably in free Colab RAM (12 GB). In-memory Qdrant only becomes impractical above ~1M chunks (~1.5 GB), at which point you would switch to persistent Qdrant with **HNSW (Hierarchical Navigable Small World)** graph indexing \u2014 a nearest-neighbour data structure that delivers sub-millisecond queries at billion-vector scale. The Python client API is identical for both in-memory and cloud deployments.\n",
    "\n",
    "**Why not a larger model like `all-mpnet-base-v2` or OpenAI `text-embedding-3-small`?**\n",
    "Larger models improve recall but break the \"no API key, runs on CPU\" constraint of this demo. The architecture is model-agnostic \u2014 swap the `SentenceTransformer` call in `embedding-model` and the `EMBED_DIM` constant in `qdrant-setup` and nothing else changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qdrant-setup",
   "metadata": {
    "id": "qdrant-setup"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hdr(\"Setting up Qdrant in-memory + indexing chunks\")\n",
    "\n",
    "COLLECTION = \"zerodha_faqs\"\n",
    "\n",
    "# In-memory Qdrant \u2014 identical API to Qdrant Cloud\n",
    "qdrant = QdrantClient(\":memory:\")\n",
    "qdrant.create_collection(\n",
    "    collection_name=COLLECTION,\n",
    "    vectors_config=VectorParams(size=EMBED_DIM, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Embed all chunks in batches\n",
    "BATCH_SIZE = 64\n",
    "texts = [c.text for c in all_chunks]\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding\"):\n",
    "    batch = texts[i : i + BATCH_SIZE]\n",
    "    embs  = embed_model.encode(batch, normalize_embeddings=True)\n",
    "    all_embeddings.extend(embs)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "ok(f\"Embeddings shape: {all_embeddings.shape}\")\n",
    "\n",
    "# Upsert into Qdrant\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=i,\n",
    "        vector=all_embeddings[i].tolist(),\n",
    "        payload={\n",
    "            \"chunk_id\":  c.chunk_id,\n",
    "            \"doc_id\":    c.doc_id,\n",
    "            \"title\":     c.title,\n",
    "            \"text\":      c.text,\n",
    "            \"full_text\": c.full_text,\n",
    "            \"url\":       c.url,\n",
    "            \"category\":  c.category,\n",
    "            \"chunk_idx\": c.chunk_idx,\n",
    "        }\n",
    "    )\n",
    "    for i, c in enumerate(all_chunks)\n",
    "]\n",
    "\n",
    "# Upload in batches of 256\n",
    "for i in tqdm(range(0, len(points), 256), desc=\"Uploading to Qdrant\"):\n",
    "    qdrant.upsert(collection_name=COLLECTION, points=points[i:i+256])\n",
    "\n",
    "collection_info = qdrant.get_collection(COLLECTION)\n",
    "ok(f\"Qdrant collection '{COLLECTION}' ready \u2014 {collection_info.points_count} vectors indexed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "---\n",
    "## 4 \u00b7 Hybrid Search \u2014 BM25 + Dense + RRF Fusion\n",
    "\n",
    "**Why not dense-only?**  \n",
    "Dense retrieval is strong on semantic similarity. It misses exact-term matches \u2014 financial terms, product names, regulatory codes. BM25 nails exact matches but fails on paraphrase. Neither alone is sufficient for a support FAQ system where users ask both `\"what is MTF\"` (semantic) and `\"MTF margin requirement\"` (keyword).\n",
    "\n",
    "**Reciprocal Rank Fusion (RRF):**  \n",
    "Score = \u03a3 1/(k + rank_i) where k=60 is a smoothing constant. RRF is rank-based \u2014 it does not require score normalization between BM25 and cosine similarity, which makes it robust without hyperparameter tuning.\n",
    "\n",
    "**Production trade-off:**  \n",
    "BM25 runs on CPU in memory. Dense retrieval hits the vector index. For 100k+ chunks, the bottleneck shifts to BM25 RAM \u2014 at that scale, replace BM25 with Elasticsearch sparse vectors or Qdrant's built-in sparse support (FastEmbed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bm25-setup",
   "metadata": {
    "id": "bm25-setup"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build BM25 index over tokenized chunk texts\n",
    "hdr(\"Building BM25 sparse index\")\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple whitespace + lowercase tokenizer. Good enough for BM25.\"\"\"\n",
    "    return re.findall(r\"[a-z0-9]+\", text.lower())\n",
    "\n",
    "tokenized_corpus = [tokenize(c.text) for c in all_chunks]\n",
    "bm25_index = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "ok(f\"BM25 index built over {len(tokenized_corpus)} chunks\")\n",
    "info(\"Average doc length (BM25): \" + str(round(bm25_index.avgdl, 1)) + \" tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-search-fn",
   "metadata": {
    "id": "hybrid-search-fn"
   },
   "outputs": [],
   "source": [
    "def dense_search(query: str, top_k: int = 30) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Query Qdrant for top_k nearest neighbours.\n",
    "    Returns list of (point_index, cosine_score).\n",
    "    \"\"\"\n",
    "    q_emb = embed_model.encode([query], normalize_embeddings=True)[0]\n",
    "    results = qdrant.query_points(\n",
    "        collection_name=COLLECTION,\n",
    "        query=q_emb.tolist(),\n",
    "        limit=top_k,\n",
    "    )\n",
    "    points = results[0] if isinstance(results, tuple) else results.points\n",
    "    return [(r.id, r.score) for r in points]\n",
    "\n",
    "\n",
    "def bm25_search(query: str, top_k: int = 30) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    BM25 retrieval. Returns list of (chunk_index, bm25_score).\n",
    "    \"\"\"\n",
    "    q_tokens = tokenize(query)\n",
    "    scores = bm25_index.get_scores(q_tokens)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(int(idx), float(scores[idx])) for idx in top_indices if scores[idx] > 0]\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    dense_hits:  List[Tuple[int, float]],\n",
    "    sparse_hits: List[Tuple[int, float]],\n",
    "    k: int = 60,\n",
    "    top_k: int = 20,\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Fuse two ranked lists using RRF.\n",
    "    k=60 is the standard smoothing constant from the original RRF paper.\n",
    "    \"\"\"\n",
    "    rrf_scores: Dict[int, float] = {}\n",
    "\n",
    "    for rank, (idx, _) in enumerate(dense_hits):\n",
    "        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (k + rank + 1)\n",
    "\n",
    "    for rank, (idx, _) in enumerate(sparse_hits):\n",
    "        rrf_scores[idx] = rrf_scores.get(idx, 0) + 1.0 / (k + rank + 1)\n",
    "\n",
    "    fused = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return fused[:top_k]\n",
    "\n",
    "\n",
    "def hybrid_search(query: str, stage1_k: int = 30) -> List[Tuple[Chunk, float]]:\n",
    "    \"\"\"\n",
    "    Full hybrid search pipeline.\n",
    "    Returns list of (Chunk, rrf_score) sorted by descending RRF score.\n",
    "    \"\"\"\n",
    "    dense_hits  = dense_search(query, top_k=stage1_k)\n",
    "    sparse_hits = bm25_search(query, top_k=stage1_k)\n",
    "    fused       = reciprocal_rank_fusion(dense_hits, sparse_hits, top_k=stage1_k)\n",
    "\n",
    "    results = []\n",
    "    for idx, score in fused:\n",
    "        chunk = all_chunks[idx]\n",
    "        results.append((chunk, score))\n",
    "    return results\n",
    "\n",
    "\n",
    "ok(\"Hybrid search functions ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fy7ul2o84dc",
   "metadata": {},
   "source": [
    "### Why the same article can appear multiple times in retrieval results\n",
    "\n",
    "This is **expected and correct behaviour** at the search layer. Here is the full chain of reasoning:\n",
    "\n",
    "**1. The pipeline is chunk-aware, not article-aware.**\n",
    "Every article is split into N chunks (typically 2\u20136 for Zerodha support articles). Each chunk is stored as a separate point in Qdrant with its own vector. `dense_search()` and `bm25_search()` both return *chunk-level* ranked lists \u2014 they have no concept of \"article\".\n",
    "\n",
    "**2. Chunks from the same article have nearly-identical embeddings.**\n",
    "`all-MiniLM-L6-v2` encodes semantic meaning. Two chunks from the same article cover overlapping content and therefore land very close together in the 384-dimensional embedding space. When you query for \"how do I withdraw funds\", all 4 chunks of the withdrawal article sit at roughly the same cosine distance from the query vector. All 4 get ranked near the top.\n",
    "\n",
    "**3. This is the right design for RRF fusion.**\n",
    "`reciprocal_rank_fusion()` needs the full chunk-level ranked lists from both dense and BM25 to compute accurate rank positions. If we deduplicated inside `dense_search()` before fusion, we would collapse the dense signal to a single rank per article \u2014 losing rank-spread information that RRF depends on to correctly weight evidence. Chunk-level retrieval \u2192 better fusion \u2192 better final ranking.\n",
    "\n",
    "**4. Deduplication happens downstream, not at search time.**\n",
    "The notebook applies a four-layer dedup strategy:\n",
    "\n",
    "| Stage | Where | What it does |\n",
    "|---|---|---|\n",
    "| Demo display | `_dedup_chunks()` in this cell | Shows one result per article for readability |\n",
    "| MMR | Section 4b | Penalises semantically redundant candidates before reranking |\n",
    "| Generation | `rag_query()` Stage 4 | Keeps only the best chunk per `doc_id` before building the prompt |\n",
    "| Prompt budget | `build_prompt()` | 150-char preview per source keeps 5 diverse articles inside Flan-T5's 512-token limit |\n",
    "\n",
    "The demo below calls `_dedup_chunks()` only for display. The underlying `hybrid_search()` intentionally returns chunk-level results so the cross-encoder and adaptive-k stages operate on the richest possible candidate pool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-search-demo",
   "metadata": {
    "id": "hybrid-search-demo"
   },
   "outputs": [],
   "source": [
    "\n",
    "def _dedup_chunks(hits, n: int = 5):\n",
    "    \"\"\"Deduplicate a ranked list by doc_id, keeping the top-scoring chunk per article.\n",
    "\n",
    "    Works for both dense hits (List[Tuple[int, float]]) and hybrid hits\n",
    "    (List[Tuple[Chunk, float]]) \u2014 distinguishes them by checking the first element type.\n",
    "    \"\"\"\n",
    "    seen, out = set(), []\n",
    "    for item in hits:\n",
    "        idx_or_chunk, score = item\n",
    "        doc_id = all_chunks[idx_or_chunk].doc_id if isinstance(idx_or_chunk, int) else idx_or_chunk.doc_id\n",
    "        if doc_id not in seen:\n",
    "            seen.add(doc_id)\n",
    "            out.append(item)\n",
    "        if len(out) >= n:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "\n",
    "# \u2500\u2500 Demo 1: semantic query \u2014 both Dense and Hybrid should do well \u2500\u2500\u2500\u2500\n",
    "hdr(\"Demo: Dense-only vs Hybrid Search\")\n",
    "DEMO_QUERY = \"how do I withdraw funds from my Zerodha account\"\n",
    "\n",
    "print(f\"Query 1 (semantic): '{DEMO_QUERY}'\\n\")\n",
    "\n",
    "# Fetch extra candidates upfront so dedup still returns 5 distinct articles\n",
    "dense_only  = dense_search(DEMO_QUERY, top_k=50)\n",
    "hybrid_hits = hybrid_search(DEMO_QUERY, stage1_k=50)\n",
    "\n",
    "print(f\"{Fore.CYAN}{'\u2500'*28} Dense-only top-5 (deduped by article) {'\u2500'*2}{Style.RESET_ALL}\")\n",
    "for rank, (idx, score) in enumerate(_dedup_chunks(dense_only), 1):\n",
    "    c = all_chunks[idx]\n",
    "    print(f\"  {rank}. [{score:.3f}] {c.title}\")\n",
    "\n",
    "print(f\"\\n{Fore.CYAN}{'\u2500'*28} Hybrid top-5 (RRF, deduped) {'\u2500'*4}{Style.RESET_ALL}\")\n",
    "for rank, (chunk, score) in enumerate(_dedup_chunks(hybrid_hits), 1):\n",
    "    print(f\"  {rank}. [{score:.4f}] {chunk.title}\")\n",
    "\n",
    "# \u2500\u2500 Demo 2: exact financial abbreviations \u2014 where BM25 wins \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "TERM_QUERY = \"what is BTST and CNC order type\"\n",
    "print(f\"\\n\\nQuery 2 (exact financial terms): '{TERM_QUERY}'\\n\")\n",
    "print(f\"{Fore.YELLOW}These abbreviations are hard for dense embeddings \u2014 BM25 recovers \"\n",
    "      f\"them via exact token match.{Style.RESET_ALL}\\n\")\n",
    "\n",
    "dense_term  = dense_search(TERM_QUERY, top_k=50)\n",
    "hybrid_term = hybrid_search(TERM_QUERY, stage1_k=50)\n",
    "\n",
    "print(f\"{Fore.CYAN}{'\u2500'*28} Dense-only top-5 (deduped by article) {'\u2500'*2}{Style.RESET_ALL}\")\n",
    "for rank, (idx, score) in enumerate(_dedup_chunks(dense_term), 1):\n",
    "    c = all_chunks[idx]\n",
    "    print(f\"  {rank}. [{score:.3f}] {c.title}\")\n",
    "\n",
    "print(f\"\\n{Fore.CYAN}{'\u2500'*28} Hybrid top-5 (RRF, deduped) {'\u2500'*4}{Style.RESET_ALL}\")\n",
    "for rank, (chunk, score) in enumerate(_dedup_chunks(hybrid_term), 1):\n",
    "    print(f\"  {rank}. [{score:.4f}] {chunk.title}\")\n",
    "\n",
    "print(f\"\\n{Fore.YELLOW}Key insight:{Style.RESET_ALL} BM25 recovers exact financial terms \"\n",
    "      \"(BTST, CNC, NRML, MTF) that dense retrieval misses due to \"\n",
    "      \"abbreviation tokenization. Hybrid is not an optimisation \u2014 it is a requirement.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eznslgnp7m5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 4b \u00b7 MMR \u2014 Maximal Marginal Relevance\n",
    "\n",
    "**The redundancy problem:** Dense retrieval on a broad query often returns 5 chunks that all say the same thing. You burn your entire LLM context window on one idea \u2014 the answer is impoverished even when recall was perfect.\n",
    "\n",
    "**MMR formula:**\n",
    "\n",
    "```\n",
    "MMR = argmax_{d \u2208 R \\ S} [ \u03bb \u00b7 sim(d, q)  \u2212  (1 \u2212 \u03bb) \u00b7 max_{d' \u2208 S} sim(d, d') ]\n",
    "```\n",
    "\n",
    "- **Left term:** relevance to the query (cosine similarity to query embedding)  \n",
    "- **Right term:** maximum similarity to any already-selected chunk (diversity penalty)  \n",
    "- **\u03bb \u2248 0.6** balances relevance vs diversity \u2014 production default  \n",
    "- **Algorithm:** greedy iterative selection \u2014 at each step pick the candidate with the best relevance-minus-redundancy score\n",
    "\n",
    "**Where it fits:** Run MMR *after* hybrid retrieval and *before* the cross-encoder. It trims the shortlist to a diverse set, so the cross-encoder scores a tighter, non-redundant candidate pool.\n",
    "\n",
    "> **Production note:** MMR replaces the diversity problem that fixed-k ignores. Combine with Adaptive-k for the full effect: MMR for diversity, Adaptive-k for relevance boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fwd8sexb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mmr(\n",
    "    query:      str,\n",
    "    candidates: List[Tuple[Chunk, float]],\n",
    "    final_k:    int   = 8,\n",
    "    lambda_:    float = 0.6,\n",
    ") -> List[Tuple[Chunk, float]]:\n",
    "    \"\"\"\n",
    "    Maximal Marginal Relevance \u2014 greedy iterative selection for diversity.\n",
    "\n",
    "    At each step selects the candidate that maximises:\n",
    "        \u03bb \u00b7 sim(chunk, query)  \u2212  (1\u2212\u03bb) \u00b7 max_{selected} sim(chunk, selected)\n",
    "\n",
    "    \u03bb=0.6 favours relevance slightly over diversity (production default).\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    texts  = [c.text for c, _ in candidates]\n",
    "    q_emb  = embed_model.encode([query], normalize_embeddings=True)          # (1, D)\n",
    "    c_embs = embed_model.encode(texts,   normalize_embeddings=True)          # (N, D)\n",
    "\n",
    "    # Relevance scores: query \u2194 each candidate\n",
    "    rel_scores = cosine_similarity(q_emb, c_embs)[0]                        # (N,)\n",
    "\n",
    "    selected_idxs:  List[int] = []\n",
    "    remaining_idxs: List[int] = list(range(len(candidates)))\n",
    "\n",
    "    for _ in range(min(final_k, len(candidates))):\n",
    "        if not remaining_idxs:\n",
    "            break\n",
    "\n",
    "        if not selected_idxs:\n",
    "            # First pick: highest relevance, no diversity penalty yet\n",
    "            best = max(remaining_idxs, key=lambda i: rel_scores[i])\n",
    "        else:\n",
    "            sel_embs = c_embs[selected_idxs]                                 # (|S|, D)\n",
    "            best, best_score = None, -float(\"inf\")\n",
    "            for i in remaining_idxs:\n",
    "                relevance  = lambda_ * rel_scores[i]\n",
    "                redundancy = (1 - lambda_) * cosine_similarity(\n",
    "                    c_embs[[i]], sel_embs\n",
    "                ).max()\n",
    "                score = relevance - redundancy\n",
    "                if score > best_score:\n",
    "                    best_score, best = score, i\n",
    "\n",
    "        selected_idxs.append(best)\n",
    "        remaining_idxs.remove(best)\n",
    "\n",
    "    return [(candidates[i][0], float(rel_scores[i])) for i in selected_idxs]\n",
    "\n",
    "\n",
    "# \u2500\u2500 Demo: before/after MMR on a redundancy-heavy query \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "MMR_QUERY = \"what is margin in Zerodha\"\n",
    "hdr(f\"Demo: MMR Diversity \u2014 '{MMR_QUERY}'\")\n",
    "\n",
    "raw_candidates = hybrid_search(MMR_QUERY, stage1_k=30)\n",
    "mmr_results    = mmr(MMR_QUERY, raw_candidates, final_k=5, lambda_=0.6)\n",
    "\n",
    "print(f\"{Fore.RED}WITHOUT MMR \u2014 top 5 by RRF score (may be redundant):{Style.RESET_ALL}\")\n",
    "for i, (chunk, score) in enumerate(raw_candidates[:5], 1):\n",
    "    snippet = chunk.text[len(chunk.title) + 2:len(chunk.title) + 90]\n",
    "    print(f\"  {i}. [{score:.4f}] {chunk.title}\")\n",
    "    print(f\"       \u2026{snippet}\u2026\")\n",
    "\n",
    "print(f\"\\n{Fore.GREEN}WITH MMR (\u03bb=0.6) \u2014 diverse top 5:{Style.RESET_ALL}\")\n",
    "for i, (chunk, score) in enumerate(mmr_results, 1):\n",
    "    snippet = chunk.text[len(chunk.title) + 2:len(chunk.title) + 90]\n",
    "    print(f\"  {i}. [{score:.4f}] {chunk.title}\")\n",
    "    print(f\"       \u2026{snippet}\u2026\")\n",
    "\n",
    "print(f\"\\n{Fore.YELLOW}Key insight:{Style.RESET_ALL} Each MMR-selected chunk covers a \"\n",
    "      \"distinct aspect of 'margin'. No two chunks say the same thing.\")\n",
    "\n",
    "ok(\"MMR function ready. Use mmr(query, hybrid_search(...)) before cross-encoder for diversity.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "---\n## 5 \u00b7 Cross-Encoder Reranking\n\n**The two-tower gap:** Bi-encoders (like MiniLM) encode query and document independently \u2014 they cannot model fine-grained token-level interactions between the two. This is fast but imprecise.\n\n**Cross-encoders** process the (query, document) pair jointly through the full attention stack. They see every token of the query attending to every token of the document. Dramatically more accurate \u2014 but O(n) in inference cost.\n\n**The production pattern:** Use bi-encoder to recall 20\u201330 candidates cheaply, then cross-encoder to rerank to final top-5. You pay cross-encoder cost only on the shortlist, not the full corpus.\n\n**Latency budget:** With `ms-marco-MiniLM-L-6-v2` on CPU:  \n- Bi-encoder recall on 2,500+ chunks (HNSW index): ~80ms  \n- Cross-encoder on 20 candidates: ~120ms  \n- Total retrieval budget: ~200ms \u2014 well inside a 1.5s response SLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-encoder",
   "metadata": {
    "id": "cross-encoder"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hdr(\"Loading Cross-Encoder reranker\")\n",
    "CE_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "cross_encoder = CrossEncoder(CE_MODEL_NAME)\n",
    "ok(f\"Cross-encoder: {CE_MODEL_NAME}\")\n",
    "\n",
    "\n",
    "def rerank(\n",
    "    query: str,\n",
    "    candidates: List[Tuple[Chunk, float]],\n",
    "    final_k: int = 5,\n",
    ") -> List[Tuple[Chunk, float]]:\n",
    "    \"\"\"\n",
    "    Rerank hybrid search candidates with the cross-encoder.\n",
    "    Input:  candidates from hybrid_search (shortlist of ~20\u201330)\n",
    "    Output: top final_k chunks with cross-encoder relevance scores\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    pairs  = [(query, c.text) for c, _ in candidates]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    ranked = sorted(\n",
    "        zip([c for c, _ in candidates], scores),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    return ranked[:final_k]\n",
    "\n",
    "\n",
    "# \u2500\u2500 Demo: per-stage latency (bi-encoder vs cross-encoder) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "print(f\"\\nQuery: '{DEMO_QUERY}'\")\n",
    "\n",
    "t0 = time.time()\n",
    "candidates = hybrid_search(DEMO_QUERY, stage1_k=25)\n",
    "t_hybrid_ms = (time.time() - t0) * 1000\n",
    "\n",
    "t0 = time.time()\n",
    "reranked = rerank(DEMO_QUERY, candidates, final_k=5)\n",
    "t_ce_ms = (time.time() - t0) * 1000\n",
    "\n",
    "print(f\"\\n  {Fore.CYAN}Stage latencies:{Style.RESET_ALL}\")\n",
    "print(f\"  Bi-encoder recall  (25 candidates):  {t_hybrid_ms:6.0f}ms\")\n",
    "print(f\"  Cross-encoder rerank (\u2192 top 5):       {t_ce_ms:6.0f}ms\")\n",
    "print(f\"  {'\u2500'*42}\")\n",
    "print(f\"  Total retrieval:                      {t_hybrid_ms + t_ce_ms:6.0f}ms\")\n",
    "print(f\"\\n  {Fore.YELLOW}Note:{Style.RESET_ALL} CE cost scales with candidate count, \"\n",
    "      f\"not corpus size. Running on 25 docs, not {len(all_chunks)}.\")\n",
    "\n",
    "print(f\"\\n{Fore.CYAN}{'\u2500'*30} After cross-encoder reranking {'\u2500'*1}{Style.RESET_ALL}\")\n",
    "for rank, (chunk, score) in enumerate(reranked, 1):\n",
    "    print(f\"  {rank}. [CE={score:.3f}] {chunk.title}\")\n",
    "    print(f\"       {chunk.text[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "---\n",
    "## 6 \u00b7 Adaptive-k Retrieval\n",
    "\n",
    "**The fixed-k problem:** Setting `top_k=5` works well for narrow queries. For ambiguous queries \u2014 `\"margin\"` could mean MTF margin, intraday margin, or Options margin \u2014 you need more context. But padding every query with top-10 inflates prompt cost and dilutes the signal fed to the LLM.\n",
    "\n",
    "**Adaptive-k** uses the distribution of cross-encoder scores to find the natural relevance cliff:\n",
    "\n",
    "```\n",
    "k = argmax( score_i \u2212 score_{i+1} )\n",
    "```\n",
    "\n",
    "The largest consecutive score drop is the boundary between relevant and irrelevant. Everything above the cliff goes into context. Everything below gets dropped \u2014 no LLM sees it, no token budget is consumed.\n",
    "\n",
    "**When it fails:** Flat score curves with no clear cliff \u2014 usually when the query is highly ambiguous or the embedding space is too compressed. Mitigate with a minimum k of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-k",
   "metadata": {
    "id": "adaptive-k"
   },
   "outputs": [],
   "source": [
    "def adaptive_k(\n",
    "    ranked_chunks: List[Tuple[Chunk, float]],\n",
    "    min_k: int = 2,\n",
    "    max_k: int = 8,\n",
    ") -> List[Tuple[Chunk, float]]:\n",
    "    \"\"\"\n",
    "    Apply adaptive-k boundary detection on cross-encoder scored candidates.\n",
    "    Returns only the chunks above the steepest score drop.\n",
    "\n",
    "    k = argmax(score_i - score_{i+1})\n",
    "    \"\"\"\n",
    "    if len(ranked_chunks) <= min_k:\n",
    "        return ranked_chunks\n",
    "\n",
    "    scores = [s for _, s in ranked_chunks[:max_k]]\n",
    "\n",
    "    # Compute consecutive differences\n",
    "    gaps = [scores[i] - scores[i+1] for i in range(len(scores)-1)]\n",
    "\n",
    "    if not gaps:\n",
    "        return ranked_chunks[:min_k]\n",
    "\n",
    "    cliff_idx = int(np.argmax(gaps))   # index of steepest drop\n",
    "    k = max(min_k, cliff_idx + 1)      # include everything above the cliff\n",
    "\n",
    "    return ranked_chunks[:k]\n",
    "\n",
    "\n",
    "# \u2500\u2500 Demo: adaptive-k vs fixed-k on several query types \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "hdr(\"Demo: Adaptive-k vs Fixed-k\")\n",
    "\n",
    "test_queries = [\n",
    "    \"how to place a stop loss order\",       # narrow \u2014 expect k=2 or 3\n",
    "    \"margin\",                                # ambiguous \u2014 expect higher k\n",
    "    \"can I withdraw money on the same day\",  # specific \u2014 expect k=2\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    candidates = hybrid_search(q, stage1_k=25)\n",
    "    reranked   = rerank(q, candidates, final_k=8)\n",
    "    adaptive   = adaptive_k(reranked, min_k=2, max_k=8)\n",
    "\n",
    "    scores = [f\"{s:.2f}\" for _, s in reranked[:8]]\n",
    "    gaps   = [\n",
    "        f\"{reranked[i][1]-reranked[i+1][1]:.2f}\"\n",
    "        for i in range(len(reranked)-1) if i < 7\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n  Query: '{q}'\")\n",
    "    print(f\"  CE scores: [{', '.join(scores)}]\")\n",
    "    print(f\"  Gaps:      [{', '.join(gaps)}]\")\n",
    "    print(f\"  {Fore.GREEN}Adaptive-k = {len(adaptive)}{Style.RESET_ALL}  \"\n",
    "          f\"(Fixed-k=5 would use {min(5, len(reranked))} chunks)\")\n",
    "    for i, (chunk, score) in enumerate(adaptive, 1):\n",
    "        print(f\"    {i}. [CE={score:.3f}] {chunk.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "---\n",
    "## 7 \u00b7 Generation with Grounded Citations\n",
    "\n",
    "We use `google/flan-t5-base` here \u2014 a small (250M param) instruction-tuned model that runs on CPU in Colab without any API key. It is not production-grade for generative quality, but it demonstrates the full pipeline architecture correctly.\n",
    "\n",
    "**To swap in a better model:** Replace the generation cell with an OpenAI/Anthropic API call \u2014 the retrieval pipeline above is completely model-agnostic. The prompt template stays identical.\n",
    "\n",
    "**Citation grounding:** Every answer cites the chunk IDs it used. If the model produces a claim not traceable to a retrieved chunk, that is your hallucination signal \u2014 catch it at evaluation time (Section 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-generator",
   "metadata": {
    "id": "load-generator"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hdr(\"Loading generation model (Flan-T5-base \u2014 CPU, no API key)\")\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "GEN_MODEL     = \"google/flan-t5-base\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
    "gen_model     = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL)\n",
    "\n",
    "def generator(prompt: str, max_new_tokens: int = 256, **kwargs) -> list:\n",
    "    # Flan-T5-base encoder hard limit is 512 tokens.\n",
    "    # Truncate at 512 (not 1024) so the model actually sees the full prompt.\n",
    "    inputs  = gen_tokenizer(prompt, return_tensors=\"pt\",\n",
    "                            truncation=True, max_length=512)\n",
    "    outputs = gen_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    text    = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return [{\"generated_text\": text}]\n",
    "\n",
    "ok(f\"Generator ready: {GEN_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-pipeline",
   "metadata": {
    "id": "rag-pipeline"
   },
   "outputs": [],
   "source": [
    "def build_prompt(query: str, chunks: List[Tuple[Chunk, float]]) -> str:\n",
    "    \"\"\"\n",
    "    Construct a grounded RAG prompt optimised for Flan-T5-base (512-token limit).\n",
    "\n",
    "    Design constraints:\n",
    "    \u2022 Question is placed FIRST \u2014 protected from truncation even if context fills the window.\n",
    "    \u2022 Each source uses max 150 chars (~40 tokens) so 5 sources fit inside the 512-token limit.\n",
    "    \u2022 Sources are labelled \"Source 1:\", \"Source 2:\" \u2014 natural text patterns Flan-T5 handles well.\n",
    "    \u2022 Citation instruction says \"mention 'Source N'\" rather than showing a bracket placeholder\n",
    "      like [SOURCE-N], which Flan-T5 echoes literally instead of filling in.\n",
    "    \u2022 Instruction is one plain sentence \u2014 seq2seq models respond best to simple directives.\n",
    "    \"\"\"\n",
    "    context_blocks = []\n",
    "    for i, (chunk, score) in enumerate(chunks, 1):\n",
    "        context_blocks.append(\n",
    "            f\"Source {i} ({chunk.title}): {chunk.text[:150]}\"\n",
    "        )\n",
    "    context = \"\\n\".join(context_blocks)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Answer using only the sources below. \"\n",
    "        f\"When you use information from a source, mention it as 'Source 1', 'Source 2', etc.\\n\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RAGResponse:\n",
    "    query:    str\n",
    "    answer:   str\n",
    "    sources:  List[Chunk]\n",
    "    latency_ms: float\n",
    "\n",
    "\n",
    "def rag_query(\n",
    "    query:      str,\n",
    "    stage1_k:   int = 25,\n",
    "    final_k:    int = 8,\n",
    "    use_adaptive_k: bool = True,\n",
    ") -> RAGResponse:\n",
    "    \"\"\"\n",
    "    Full production RAG pipeline:\n",
    "    Hybrid Search \u2192 Cross-Encoder Rerank \u2192 Adaptive-k \u2192 Doc-dedup \u2192 Generate\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Stage 1: Hybrid retrieval\n",
    "    candidates = hybrid_search(query, stage1_k=stage1_k)\n",
    "\n",
    "    # Stage 2: Cross-encoder reranking\n",
    "    reranked = rerank(query, candidates, final_k=final_k)\n",
    "\n",
    "    # Stage 3: Adaptive-k boundary detection\n",
    "    if use_adaptive_k:\n",
    "        final_chunks = adaptive_k(reranked, min_k=2, max_k=final_k)\n",
    "    else:\n",
    "        final_chunks = reranked[:5]\n",
    "\n",
    "    # Stage 4: Deduplicate by doc_id \u2014 keep only the highest-scoring chunk per\n",
    "    # article so the same article doesn't crowd out all source slots.\n",
    "    seen_docs: set = set()\n",
    "    deduped: List[Tuple[Chunk, float]] = []\n",
    "    for chunk, score in final_chunks:\n",
    "        if chunk.doc_id not in seen_docs:\n",
    "            seen_docs.add(chunk.doc_id)\n",
    "            deduped.append((chunk, score))\n",
    "    final_chunks = deduped\n",
    "\n",
    "    # Stage 5: Generate grounded answer\n",
    "    prompt = build_prompt(query, final_chunks)\n",
    "    output = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "    latency_ms = (time.time() - t0) * 1000\n",
    "\n",
    "    return RAGResponse(\n",
    "        query=query,\n",
    "        answer=output,\n",
    "        sources=[c for c, _ in final_chunks],\n",
    "        latency_ms=latency_ms,\n",
    "    )\n",
    "\n",
    "\n",
    "ok(\"RAG pipeline assembled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u3gvv2pb4u8",
   "metadata": {},
   "source": [
    "### Prompt design for Flan-T5-base: three non-obvious constraints\n",
    "\n",
    "Flan-T5-base is a seq2seq model instruction-tuned on hundreds of NLP tasks. It is not a GPT-style autoregressive decoder \u2014 it has a separate encoder and decoder, and the encoder has a **hard 512-token limit**. This shapes every design decision in `build_prompt()` and `rag_query()`.\n",
    "\n",
    "**Constraint 1: Question must come first.**\n",
    "The encoder tokenises the full prompt left-to-right and truncates at 512 tokens. If the prompt is structured as `SOURCES \u2026 QUESTION`, a long context will fill the 512-token window before the QUESTION line is reached \u2014 the model never sees what it is being asked, and answers with something plausible from the sources regardless of the query. Putting `Question: {query}` at the top ensures the question survives truncation even in the worst case.\n",
    "\n",
    "**Constraint 2: Token budget arithmetic.**\n",
    "With the question-first layout the token budget is approximately:\n",
    "\n",
    "| Component | Tokens |\n",
    "|---|---|\n",
    "| Instruction line | ~20 |\n",
    "| Question (avg Zerodha query) | ~15 |\n",
    "| 5 sources \u00d7 (title + 150 chars) | ~250 |\n",
    "| Formatting / labels | ~15 |\n",
    "| **Total** | **~300** |\n",
    "\n",
    "This leaves a 200-token safety margin under the 512-token limit. The 150-char preview in `build_prompt()` is calibrated to this budget. Increasing it to 300 chars with 5 sources would hit ~430 tokens \u2014 still safe, but with less headroom.\n",
    "\n",
    "**Constraint 3: Doc-id deduplication before generation.**\n",
    "`adaptive_k()` returns chunk-level candidates. Without deduplication, the same article can fill 3 of the 5 source slots \u2014 the model then sees the same information three times and the remaining slots are wasted on redundant context. The dedup step in `rag_query()` Stage 4 ensures each source slot represents a distinct article, maximising the information density of the prompt.\n",
    "\n",
    "**Swapping Flan-T5 for a real LLM.**\n",
    "`build_prompt()` and `RAGResponse` are completely LLM-agnostic. To use Claude or GPT-4.1-mini:\n",
    "1. Replace the `generator()` function in `load-generator` with an API call.\n",
    "2. Increase the 150-char preview limit \u2014 modern LLMs handle 8k\u2013128k context windows.\n",
    "3. Everything else (hybrid search, reranking, adaptive-k, dedup, evaluation) stays identical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-demo",
   "metadata": {
    "id": "rag-demo"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hdr(\"Running RAG Pipeline \u2014 Live Queries\")\n",
    "\n",
    "demo_queries = [\n",
    "    \"How do I withdraw money from Zerodha to my bank account?\",\n",
    "    \"What is the difference between CNC and MIS orders?\",\n",
    "    \"Why was my F&O trade rejected due to insufficient margin?\",\n",
    "    \"How do I add a nominee to my Zerodha account?\",\n",
    "]\n",
    "\n",
    "responses: List[RAGResponse] = []\n",
    "\n",
    "for q in demo_queries:\n",
    "    print(f\"\\n{Fore.YELLOW}Q: {q}{Style.RESET_ALL}\")\n",
    "    resp = rag_query(q)\n",
    "    responses.append(resp)\n",
    "\n",
    "    print(f\"{Fore.GREEN}A: {resp.answer}{Style.RESET_ALL}\")\n",
    "    print(f\"   Latency: {resp.latency_ms:.0f}ms | Sources used: {len(resp.sources)}\")\n",
    "    for i, src in enumerate(resp.sources, 1):\n",
    "        print(f\"   [{i}] {src.title} \u2192 {src.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "---\n",
    "## 8 \u00b7 Evaluation \u2014 Hit Rate, Faithfulness, Hallucination Detection\n",
    "\n",
    "**The gap most teams skip:** A RAG system that looks good in demos can still fail silently in production. Two failure modes from the deck:\n",
    "\n",
    "1. **Bad retrieval** \u2014 the right chunk was not retrieved at all. No amount of generation quality fixes this. Measure: Hit Rate @ k.\n",
    "2. **Bad generation** \u2014 the chunk was retrieved but the model hallucinated or ignored it. Measure: Faithfulness score.\n",
    "\n",
    "We use a **synthetic golden QA set** generated from the corpus itself \u2014 no human annotation required for a demo. In production, seed this with real user queries from your support ticket logs.\n",
    "\n",
    "**Faithfulness check:** A simple heuristic \u2014 does the answer contain n-grams that appear in the retrieved source? Not a replacement for LLM-as-judge, but deterministic and fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-qa",
   "metadata": {
    "id": "golden-qa"
   },
   "outputs": [],
   "source": [
    "hdr(\"Building Synthetic Golden QA Set\")\n",
    "\n",
    "# Hand-crafted query \u2192 expected article title pairs covering all 32 scraped sub-categories.\n",
    "# expected_titles: lowercase substrings matched against retrieved article titles.\n",
    "# In production: derive from support ticket logs or LLM-generated QA pairs.\n",
    "#\n",
    "# Section / sub-category coverage:\n",
    "#   funds/fund-withdrawal            \u2192 Q01, Q02\n",
    "#   funds/adding-funds               \u2192 Q03\n",
    "#   funds/adding-bank-accounts       \u2192 Q04\n",
    "#   funds/mandate                    \u2192 Q05\n",
    "#   trading/trading-faqs             \u2192 Q06, Q07, Q08, Q09\n",
    "#   trading/margins                  \u2192 Q10, Q11, Q12\n",
    "#   trading/charts-and-orders        \u2192 Q13, Q14, Q15, Q16, Q17\n",
    "#   trading/general-kite             \u2192 Q18, Q19\n",
    "#   trading/charges                  \u2192 Q20, Q21\n",
    "#   trading/ipo                      \u2192 Q22, Q23\n",
    "#   trading/alerts-and-nudges        \u2192 Q24\n",
    "#   account-opening/resident         \u2192 Q25, Q26, Q27\n",
    "#   account-opening/minor            \u2192 Q28\n",
    "#   account-opening/nri              \u2192 Q29\n",
    "#   account-opening/corporate-huf    \u2192 Q30\n",
    "#   your-account/your-profile        \u2192 Q31, Q32\n",
    "#   your-account/account-modification\u2192 Q33\n",
    "#   your-account/dp-bank-details     \u2192 Q34, Q35\n",
    "#   your-account/nomination          \u2192 Q36\n",
    "#   your-account/transfer-of-shares  \u2192 Q37\n",
    "#   console/portfolio                \u2192 Q38\n",
    "#   console/corporate-actions        \u2192 Q39\n",
    "#   console/ledger                   \u2192 Q40\n",
    "#   console/reports                  \u2192 Q41, Q42\n",
    "#   console/profile                  \u2192 Q43\n",
    "#   console/segments                 \u2192 Q44\n",
    "#   mutual-funds/understanding-mf    \u2192 Q45, Q46, Q47\n",
    "#   mutual-funds/payments-and-orders \u2192 Q48, Q49\n",
    "#   mutual-funds/nps                 \u2192 Q50\n",
    "#   mutual-funds/fixed-deposits      \u2192 Q51\n",
    "#   mutual-funds/features-on-coin    \u2192 Q52\n",
    "#   mutual-funds/coin-general        \u2192 Q53\n",
    "GOLDEN_QA = [\n",
    "    # \u2500\u2500 funds/fund-withdrawal \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I withdraw money from Zerodha?\",\n",
    "     \"expected_titles\": [\"fund withdrawal\", \"withdraw\", \"payout\"]},\n",
    "\n",
    "    {\"query\": \"What is the cut-off time for fund withdrawal in Zerodha?\",\n",
    "     \"expected_titles\": [\"withdrawal\", \"payout\", \"cut-off\", \"timing\"]},\n",
    "\n",
    "    # \u2500\u2500 funds/adding-funds \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I add funds to my Zerodha trading account?\",\n",
    "     \"expected_titles\": [\"add funds\", \"deposit\", \"transfer funds\", \"upi\"]},\n",
    "\n",
    "    # \u2500\u2500 funds/adding-bank-accounts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I link a bank account to my Zerodha account?\",\n",
    "     \"expected_titles\": [\"bank account\", \"add bank\", \"primary bank\"]},\n",
    "\n",
    "    # \u2500\u2500 funds/mandate \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"What is a NACH mandate and why is it required in Zerodha?\",\n",
    "     \"expected_titles\": [\"mandate\", \"nach\", \"auto-debit\", \"auto debit\"]},\n",
    "\n",
    "    # \u2500\u2500 trading-and-markets/trading-faqs \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"What is the difference between holdings and positions?\",\n",
    "     \"expected_titles\": [\"holdings and positions\", \"difference between holdings\"]},\n",
    "\n",
    "    {\"query\": \"What is F&O trading?\",\n",
    "     \"expected_titles\": [\"futures and options\", \"f&o\", \"derivatives\"]},\n",
    "\n",
    "    {\"query\": \"What is the settlement process for equity trades?\",\n",
    "     \"expected_titles\": [\"settlement\", \"t+1\", \"delivery\"]},\n",
    "\n",
    "    {\"query\": \"What is BTST trading and is it allowed in Zerodha?\",\n",
    "     \"expected_titles\": [\"btst\", \"buy today sell tomorrow\"]},\n",
    "\n",
    "    # \u2500\u2500 trading-and-markets/margins \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"What is MTF and how does it work?\",\n",
    "     \"expected_titles\": [\"mtf\", \"margin trading facility\"]},\n",
    "\n",
    "    {\"query\": \"What are intraday margins?\",\n",
    "     \"expected_titles\": [\"intraday\", \"margin\", \"mis\"]},\n",
    "\n",
    "    {\"query\": \"What is span and exposure margin in F&O?\",\n",
    "     \"expected_titles\": [\"span\", \"exposure\", \"margin\", \"f&o\"]},\n",
    "\n",
    "    # \u2500\u2500 trading-and-markets/charts-and-orders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"What is a stop loss order?\",\n",
    "     \"expected_titles\": [\"stop loss\", \"sl order\"]},\n",
    "\n",
    "    {\"query\": \"What is a GTT order and how do I place one?\",\n",
    "     \"expected_titles\": [\"gtt\", \"good till triggered\", \"good-till-triggered\"]},\n",
    "\n",
    "    {\"query\": \"What is CNC order type in Zerodha?\",\n",
    "     \"expected_titles\": [\"cnc\", \"cash and carry\", \"delivery\"]},\n",
    "\n",
    "    {\"query\": \"How do I place an after-market order on Kite?\",\n",
    "     \"expected_titles\": [\"amo\", \"after market\", \"after-market\", \"pre-market\"]},\n",
    "\n",
    "    {\"query\": \"How do I convert an MIS position to CNC in Zerodha?\",\n",
    "     \"expected_titles\": [\"mis\", \"cnc\", \"convert\", \"position conversion\"]},\n",
    "\n",
    "    # \u2500\u2500 trading-and-markets/general-kite \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How to create a support ticket on Zerodha?\",\n",
    "     \"expected_titles\": [\"ticket\", \"create a ticket\", \"raise a ticket\"]},\n",
    "\n",
    "    {\"query\": \"What is Kite and how do I use it for trading?\",\n",
    "     \"expected_titles\": [\"kite\", \"trading platform\", \"web platform\"]},\n",
    "\n",
    "    # \u2500\u2500 trading-and-markets/charges \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"What are the brokerage charges for equity and F&O trades?\",\n",
    "     \"expected_titles\": [\"brokerage\", \"charges\", \"stt\", \"fee\"]},\n",
    "\n",
    "    {\"query\": \"What is STT (Securities Transaction Tax) in Zerodha?\",\n",
    "     \"expected_titles\": [\"stt\", \"securities transaction tax\", \"tax\"]},\n",
    "\n",
    "    # \u2500\u2500 trading-and-markets/ipo \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I apply for an IPO through Zerodha?\",\n",
    "     \"expected_titles\": [\"ipo\", \"apply for ipo\", \"asba\", \"upi\"]},\n",
    "\n",
    "    {\"query\": \"How do I check my IPO allotment status?\",\n",
    "     \"expected_titles\": [\"ipo\", \"allotment\", \"allotted\"]},\n",
    "\n",
    "    # \u2500\u2500 trading-and-markets/alerts-and-nudges \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I set price alerts on Kite?\",\n",
    "     \"expected_titles\": [\"alert\", \"price alert\", \"nudge\"]},\n",
    "\n",
    "    # \u2500\u2500 account-opening/resident-individual \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"What documents are required to open a Zerodha account?\",\n",
    "     \"expected_titles\": [\"documents\", \"open an account\", \"account opening\", \"kyc\"]},\n",
    "\n",
    "    {\"query\": \"How long does it take to open a Zerodha account online?\",\n",
    "     \"expected_titles\": [\"open account\", \"account opening\", \"days\", \"time\"]},\n",
    "\n",
    "    {\"query\": \"Is Aadhaar mandatory to open a Zerodha account?\",\n",
    "     \"expected_titles\": [\"aadhaar\", \"account opening\", \"mandatory\", \"digilocker\"]},\n",
    "\n",
    "    # \u2500\u2500 account-opening/minor \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"Can a minor open a trading account in Zerodha?\",\n",
    "     \"expected_titles\": [\"minor\", \"guardian\", \"minor account\"]},\n",
    "\n",
    "    # \u2500\u2500 account-opening/nri-account-opening \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"Can NRIs trade on the Indian stock market through Zerodha?\",\n",
    "     \"expected_titles\": [\"nri\", \"non-resident\", \"nre\", \"nro\"]},\n",
    "\n",
    "    # \u2500\u2500 account-opening/company-partnership-and-huf \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I open a trading account for a company or HUF?\",\n",
    "     \"expected_titles\": [\"company\", \"huf\", \"partnership\", \"corporate account\"]},\n",
    "\n",
    "    # \u2500\u2500 your-zerodha-account/your-profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I change my mobile number linked to my Zerodha account?\",\n",
    "     \"expected_titles\": [\"mobile\", \"phone number\", \"change mobile\", \"update\"]},\n",
    "\n",
    "    {\"query\": \"How do I update my email address on Zerodha?\",\n",
    "     \"expected_titles\": [\"email\", \"update email\", \"change email\"]},\n",
    "\n",
    "    # \u2500\u2500 your-zerodha-account/account-modification-and-segment-addition \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I add the F&O segment to my existing Zerodha account?\",\n",
    "     \"expected_titles\": [\"segment\", \"f&o\", \"add segment\", \"activation\", \"futures\"]},\n",
    "\n",
    "    # \u2500\u2500 your-zerodha-account/dp-id-and-bank-details \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"What is DP ID and client ID in Zerodha?\",\n",
    "     \"expected_titles\": [\"dp id\", \"client id\", \"bo id\", \"depository\"]},\n",
    "\n",
    "    {\"query\": \"How to add a bank account in Zerodha?\",\n",
    "     \"expected_titles\": [\"add bank\", \"bank account\", \"primary bank\"]},\n",
    "\n",
    "    # \u2500\u2500 your-zerodha-account/nomination-process \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I add a nominee to my Zerodha account?\",\n",
    "     \"expected_titles\": [\"nominee\", \"nomination\", \"add nominee\"]},\n",
    "\n",
    "    # \u2500\u2500 your-zerodha-account/transfer-of-shares \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I transfer shares from my Zerodha demat account to another broker?\",\n",
    "     \"expected_titles\": [\"transfer\", \"shares\", \"demat\", \"off-market\"]},\n",
    "\n",
    "    # \u2500\u2500 console/portfolio \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I view my stock portfolio on Zerodha Console?\",\n",
    "     \"expected_titles\": [\"portfolio\", \"console\", \"holdings\"]},\n",
    "\n",
    "    # \u2500\u2500 console/corporate-actions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"Where can I see dividends and bonuses credited to my Zerodha account?\",\n",
    "     \"expected_titles\": [\"dividend\", \"corporate action\", \"bonus\", \"split\"]},\n",
    "\n",
    "    # \u2500\u2500 console/ledger \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I download my account ledger from Zerodha?\",\n",
    "     \"expected_titles\": [\"ledger\", \"download\", \"statement\"]},\n",
    "\n",
    "    # \u2500\u2500 console/reports \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I download my profit and loss report for tax filing?\",\n",
    "     \"expected_titles\": [\"profit and loss\", \"p&l\", \"tax\", \"report\"]},\n",
    "\n",
    "    {\"query\": \"Where can I find my contract notes in Zerodha?\",\n",
    "     \"expected_titles\": [\"contract note\", \"trade confirmation\", \"report\"]},\n",
    "\n",
    "    # \u2500\u2500 console/profile \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I update my profile details on Zerodha Console?\",\n",
    "     \"expected_titles\": [\"profile\", \"console\", \"update\", \"details\"]},\n",
    "\n",
    "    # \u2500\u2500 console/segments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I activate the commodity or currency segment in Zerodha?\",\n",
    "     \"expected_titles\": [\"segment\", \"activate\", \"commodity\", \"currency\"]},\n",
    "\n",
    "    # \u2500\u2500 mutual-funds/understanding-mutual-funds \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I invest in mutual funds on Coin?\",\n",
    "     \"expected_titles\": [\"mutual fund\", \"coin\", \"invest\"]},\n",
    "\n",
    "    {\"query\": \"What is NAV in a mutual fund?\",\n",
    "     \"expected_titles\": [\"nav\", \"net asset value\", \"mutual fund\"]},\n",
    "\n",
    "    {\"query\": \"What is the difference between direct and regular mutual fund plans?\",\n",
    "     \"expected_titles\": [\"direct plan\", \"regular plan\", \"expense ratio\", \"direct vs regular\"]},\n",
    "\n",
    "    # \u2500\u2500 mutual-funds/payments-and-orders \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I set up a SIP on Zerodha Coin?\",\n",
    "     \"expected_titles\": [\"sip\", \"systematic investment\", \"sip on coin\"]},\n",
    "\n",
    "    {\"query\": \"How do I redeem or exit a mutual fund on Zerodha Coin?\",\n",
    "     \"expected_titles\": [\"redeem\", \"withdraw\", \"exit\", \"mutual fund\"]},\n",
    "\n",
    "    # \u2500\u2500 mutual-funds/nps \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"How do I invest in NPS (National Pension System) through Zerodha?\",\n",
    "     \"expected_titles\": [\"nps\", \"national pension\", \"pension\"]},\n",
    "\n",
    "    # \u2500\u2500 mutual-funds/fixed-deposits \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"Can I invest in fixed deposits through Zerodha?\",\n",
    "     \"expected_titles\": [\"fixed deposit\", \"fd\", \"bajaj\", \"deposit\"]},\n",
    "\n",
    "    # \u2500\u2500 mutual-funds/features-on-coin \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"What is the basket feature on Zerodha Coin?\",\n",
    "     \"expected_titles\": [\"basket\", \"coin\", \"feature\"]},\n",
    "\n",
    "    # \u2500\u2500 mutual-funds/coin-general \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    {\"query\": \"Is investing in mutual funds on Zerodha Coin free of charge?\",\n",
    "     \"expected_titles\": [\"commission\", \"direct\", \"free\", \"coin\", \"charge\"]},\n",
    "]\n",
    "\n",
    "ok(f\"Golden QA set: {len(GOLDEN_QA)} queries across all 32 scraped sub-categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-metrics",
   "metadata": {
    "id": "eval-metrics"
   },
   "outputs": [],
   "source": [
    "def hit_at_k(\n",
    "    retrieved_chunks: List[Chunk],\n",
    "    expected_title_fragments: List[str],\n",
    "    k: int = 5,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if any of the top-k retrieved chunks match\n",
    "    at least one expected title fragment (case-insensitive substring).\n",
    "    \"\"\"\n",
    "    top_titles = [c.title.lower() for c in retrieved_chunks[:k]]\n",
    "    for frag in expected_title_fragments:\n",
    "        for title in top_titles:\n",
    "            if frag.lower() in title:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def faithfulness_score(answer: str, source_chunks: List[Chunk]) -> float:\n",
    "    \"\"\"\n",
    "    Heuristic faithfulness check.\n",
    "    Counts what fraction of answer trigrams appear in the source corpus.\n",
    "\n",
    "    Not a replacement for LLM-as-judge \u2014 but deterministic, fast, and\n",
    "    surprisingly effective at catching severe hallucinations.\n",
    "    \"\"\"\n",
    "    def ngrams(text: str, n: int = 3) -> set:\n",
    "        tokens = re.findall(r\"[a-z0-9]+\", text.lower())\n",
    "        return {tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)}\n",
    "\n",
    "    answer_ngrams = ngrams(answer)\n",
    "    if not answer_ngrams:\n",
    "        return 0.0\n",
    "\n",
    "    source_text = \" \".join(c.text for c in source_chunks)\n",
    "    source_ngrams = ngrams(source_text)\n",
    "\n",
    "    overlap = answer_ngrams & source_ngrams\n",
    "    return len(overlap) / len(answer_ngrams)\n",
    "\n",
    "\n",
    "ok(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pfsajylmr4",
   "metadata": {},
   "source": [
    "### What hit@k and faithfulness actually measure \u2014 and what they miss\n",
    "\n",
    "**Hit Rate @ k (retrieval metric)**\n",
    "A query \"hits\" if at least one of the top-k retrieved chunks comes from an article whose title substring-matches an `expected_titles` entry in `GOLDEN_QA`. This tells you whether the retrieval stage surfaces the right *document* \u2014 it says nothing about whether the answer extracted from that document is correct.\n",
    "\n",
    "*Blind spot:* A pipeline that always retrieves the right article but generates a hallucinated answer will score 100% hit rate with 0% factual accuracy. Hit rate is necessary but not sufficient.\n",
    "\n",
    "**Faithfulness score (grounding metric)**\n",
    "Computed as the fraction of answer trigrams (3-word sequences) that appear verbatim in the retrieved source text. A score of 1.0 means every phrase in the answer came directly from the sources. A score of 0.3 means the model is generating content not grounded in context \u2014 likely hallucinating.\n",
    "\n",
    "*Why trigrams, not exact match?* Single words are too coarse (common words match trivially). Full sentences are too strict (the model may paraphrase correctly). Trigrams strike a balance that correlates well with human judgements of faithfulness at low computational cost.\n",
    "\n",
    "*Blind spot:* A model that copies source text verbatim scores 1.0 even if the copied text does not actually answer the question. Faithfulness measures grounding, not answer correctness. You need both.\n",
    "\n",
    "**Why synthetic golden QA?**\n",
    "Collecting real user queries with verified correct answers from a live support system takes months. Synthetic QA lets us evaluate the pipeline immediately with reasonable coverage. The 10 questions in `GOLDEN_QA` were written to cover every major Zerodha support category and to include edge cases (abbreviations, multi-step answers, regulatory questions).\n",
    "\n",
    "*The key limitation:* the synthetic questions are written by someone who already knows what's in the corpus. Real user queries are messier, more ambiguous, and often combine multiple intents. Synthetic evaluation scores are optimistic estimates of real-world performance.\n",
    "\n",
    "**For production evaluation, consider:**\n",
    "- **RAGAS** \u2014 LLM-as-judge framework that measures answer correctness, context precision, and context recall\n",
    "- **TruLens** \u2014 RAG triad (groundedness, answer relevance, context relevance) with per-query breakdown\n",
    "- **Human eval spot-check** \u2014 50 random queries judged by a domain expert remains the gold standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-eval",
   "metadata": {
    "id": "run-eval"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hdr(\"Running Evaluation \u2014 Production RAG vs Naive Dense-Only\")\n",
    "\n",
    "# \u2500\u2500 Production pipeline (Hybrid + Rerank + Adaptive-k) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "prod_results = []\n",
    "for qa in tqdm(GOLDEN_QA, desc=\"Production RAG\"):\n",
    "    candidates = hybrid_search(qa[\"query\"], stage1_k=25)\n",
    "    reranked   = rerank(qa[\"query\"], candidates, final_k=8)\n",
    "    final      = adaptive_k(reranked, min_k=2, max_k=8)\n",
    "    hit        = hit_at_k([c for c, _ in final], qa[\"expected_titles\"], k=5)\n",
    "    prompt     = build_prompt(qa[\"query\"], final)\n",
    "    answer     = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "    faith      = faithfulness_score(answer, [c for c, _ in final])\n",
    "    prod_results.append({\"hit\": hit, \"faithfulness\": faith,\n",
    "                         \"k_used\": len(final), \"answer\": answer})\n",
    "\n",
    "# \u2500\u2500 Naive baseline (dense-only, fixed top-5) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "naive_results = []\n",
    "for qa in tqdm(GOLDEN_QA, desc=\"Naive Dense-only\"):\n",
    "    dense_hits = dense_search(qa[\"query\"], top_k=5)\n",
    "    chunks     = [all_chunks[idx] for idx, _ in dense_hits]\n",
    "    hit        = hit_at_k(chunks, qa[\"expected_titles\"], k=5)\n",
    "    prompt     = build_prompt(qa[\"query\"], [(c, 0.0) for c in chunks])\n",
    "    answer     = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "    faith      = faithfulness_score(answer, chunks)\n",
    "    naive_results.append({\"hit\": hit, \"faithfulness\": faith,\n",
    "                          \"k_used\": 5, \"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-summary",
   "metadata": {
    "id": "eval-summary"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hdr(\"Evaluation Results\")\n",
    "\n",
    "prod_hit_rate = sum(r[\"hit\"] for r in prod_results) / len(prod_results)\n",
    "prod_faith    = sum(r[\"faithfulness\"] for r in prod_results) / len(prod_results)\n",
    "prod_avg_k    = sum(r[\"k_used\"] for r in prod_results) / len(prod_results)\n",
    "\n",
    "naive_hit_rate = sum(r[\"hit\"] for r in naive_results) / len(naive_results)\n",
    "naive_faith    = sum(r[\"faithfulness\"] for r in naive_results) / len(naive_results)\n",
    "\n",
    "print(f\"{'Metric':<35} {'Naive Dense-only':>18} {'Production RAG':>18}\")\n",
    "print(\"\u2500\" * 73)\n",
    "print(f\"{'Hit Rate @ 5':<35} {naive_hit_rate:>17.1%} {prod_hit_rate:>17.1%}\")\n",
    "print(f\"{'Faithfulness (trigram overlap)':<35} {naive_faith:>17.2f} {prod_faith:>17.2f}\")\n",
    "print(f\"{'Avg chunks sent to LLM':<35} {'5 (fixed)':>18} {prod_avg_k:>17.1f}\")\n",
    "print(\"\u2500\" * 73)\n",
    "print()\n",
    "\n",
    "# Per-query breakdown\n",
    "print(f\"{'Query':<52} {'Naive Hit':>11} {'Prod Hit':>10} {'Prod k':>8}\")\n",
    "print(\"\u2500\" * 84)\n",
    "for i, qa in enumerate(GOLDEN_QA):\n",
    "    q = qa[\"query\"][:50]\n",
    "    nh = Fore.GREEN + \"\u2713\" + Style.RESET_ALL if naive_results[i][\"hit\"] else Fore.RED + \"\u2717\" + Style.RESET_ALL\n",
    "    ph = Fore.GREEN + \"\u2713\" + Style.RESET_ALL if prod_results[i][\"hit\"]  else Fore.RED + \"\u2717\" + Style.RESET_ALL\n",
    "    pk = prod_results[i][\"k_used\"]\n",
    "    print(f\"  {q:<60} {nh:>9} {ph:>19} {pk:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {
    "id": "section-9"
   },
   "source": [
    "---\n",
    "## 9 \u00b7 Interactive Query Interface\n",
    "\n",
    "Type any question about Zerodha below and the full production pipeline runs \u2014 Hybrid Search \u2192 Cross-Encoder Rerank \u2192 Adaptive-k \u2192 Grounded Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive",
   "metadata": {
    "id": "interactive"
   },
   "outputs": [],
   "source": [
    "def ask(query: str, verbose: bool = True) -> RAGResponse:\n    \"\"\"\n    Ask any question. Full pipeline runs automatically.\n    Set verbose=False to suppress source details.\n    \"\"\"\n    resp = rag_query(query)\n\n    print(f\"\\n{Fore.YELLOW}{'\u2500'*60}\")\n    print(f\"Q: {query}\")\n    print(f\"{'\u2500'*60}{Style.RESET_ALL}\")\n    print(f\"\\n{Fore.GREEN}{resp.answer}{Style.RESET_ALL}\")\n    print(f\"\\n  Pipeline: Hybrid Search \u2192 CE Rerank \u2192 Adaptive-k={len(resp.sources)}\")\n    print(f\"  Latency:  {resp.latency_ms:.0f}ms\")\n\n    if verbose:\n        print(f\"\\n  {Fore.CYAN}Sources:{Style.RESET_ALL}\")\n        for i, src in enumerate(resp.sources, 1):\n            print(f\"    [{i}] {src.title}\")\n            print(f\"         {src.url}\")\n\n    return resp\n\n\n# \u2500\u2500 Try it \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Modify this query to test any Zerodha support question.\n# This default covers account-opening:\n_ = ask(\"What documents are required to open a Zerodha account?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "more-queries",
   "metadata": {
    "id": "more-queries"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# \u2500\u2500 10 pre-loaded example queries \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Covering: F&O, funds, orders, account management, compliance, charges\n",
    "# Run any single query or loop through all of them.\n",
    "\n",
    "example_queries = [\n",
    "    # Funds & withdrawals\n",
    "    \"How long does it take for a fund withdrawal to reach my bank?\",\n",
    "    \"How do I add money to my Zerodha trading account?\",\n",
    "    # Orders & position management\n",
    "    \"What happens to my F&O position on expiry day?\",\n",
    "    \"What is a GTT order and how do I place one?\",\n",
    "    \"How do I convert a CNC position to MIS intraday?\",\n",
    "    \"What is BTST trading and how does it work?\",\n",
    "    # Charges & brokerage\n",
    "    \"What is the brokerage charged on intraday equity trades?\",\n",
    "    \"What are the charges for options trading on Zerodha?\",\n",
    "    # Compliance & account management\n",
    "    \"Can I trade on a Zerodha NRI account?\",\n",
    "    \"What documents do I need to open a Zerodha demat account?\",\n",
    "]\n",
    "\n",
    "# \u2500\u2500 Run a single query (change index 0\u20139) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "_ = ask(example_queries[0])\n",
    "\n",
    "# \u2500\u2500 Uncomment to run all 10 queries \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# for q in example_queries:\n",
    "#     _ = ask(q, verbose=False)\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-notes",
   "metadata": {
    "id": "closing-notes"
   },
   "source": [
    "\n",
    "---\n",
    "## Key Takeaways\n",
    "\n",
    "```\n",
    "Production RAG is not:\n",
    "  Chunk \u2192 Embed \u2192 top-k \u2192 Generate\n",
    "\n",
    "Production RAG is:\n",
    "  Semantic Chunk (sentence-aware, overlapping)\n",
    "  \u2192 Hybrid Retrieval (BM25 + Dense, fused with RRF)\n",
    "  \u2192 MMR (Maximal Marginal Relevance \u2014 diversity over redundancy)\n",
    "  \u2192 Cross-Encoder Rerank (precision pass on shortlist)\n",
    "  \u2192 Adaptive-k (score-gap boundary \u2014 no fixed k)\n",
    "  \u2192 Grounded Generation (citations, no hallucination budget)\n",
    "  \u2192 Continuous Eval (Hit Rate + Faithfulness per release)\n",
    "```\n",
    "\n",
    "**Swap out for production:**\n",
    "\n",
    "| This notebook | Production equivalent |\n",
    "|---------------|----------------------|\n",
    "| `QdrantClient(':memory:')` | Qdrant Cloud / self-hosted |\n",
    "| `flan-t5-base` | GPT-4.1-mini / Claude 3.5 / Gemini |\n",
    "| `all-MiniLM-L6-v2` | Domain-fine-tuned embedding model |\n",
    "| `BM25Okapi` in RAM | Elasticsearch sparse / Qdrant FastEmbed |\n",
    "| `mmr()` in-process | Qdrant native MMR / Cohere Rerank diversity mode |\n",
    "| Synthetic QA eval | Real user query logs + human annotation |\n",
    "\n",
    "**Further reading:** HyDE \u00b7 HyPE \u00b7 Contextual Chunk Headers \u00b7 Fusion Retrieval \u00b7 PageRank in RAG  \n",
    "\n",
    "**Github\u2192** `github.com/indranildchandra/rag-done-right-in-production`\n",
    "\n",
    "---\n",
    "*Indranil Chandra \u00b7 ML & Data Architect, Upstox \u00b7 Co-organiser, GDG MAD Mumbai*\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}